% !Mode:: "TeX:UTF-8"
% !TeX spellcheck = zh_CN 

\chapter{绪论}[Introduction]\label{chp:intro}

\section{课题背景及意义}[Background and Significance]

\subsection{课题背景}[Background]

\textit{词向量}\cite{DBLP:journals/corr/MikolovSCCD13,pennington-socher-manning:2014:EMNLP2014}
是基于深度学习的自然语言处理的基础。
词向量为自然语言的最小语义单元 --- 词提供了包含句法语义信息的稠密的表示。
通过使用词向量作为深度神经网络的输入，
一个词自身的属性（如：词性\cite{DBLP:journals/corr/HuangXY15}），
一段词之间的关系（如：命名实体\cite{lample-EtAl:2016:N16-1}），
两个词之间的关系（如：句法\cite{dyer-EtAl:2015:ACL-IJCNLP,DBLP:journals/corr/DozatM16}）
以及一串词组成的句子、篇章的语义主旨\cite{kim-rush:2016:EMNLP2016}
等都可以得到建模。
词向量对于深度学习在自然语言处理的广泛应用起到了重要的作用。

向量化一个词的语言学基础是词义的分布式假设（distributional hypothesis）\cite{firth57synopsis}，
即：
\textit{一个词的词义可以采用它的上下文进行表示}。
现今流行的词的向量化方法进一步对这一假设进行了简化。
在这种简化中，
\textit{一个词有唯一的向量表示}并且这种表示抽象了这个词在不同上下文环境中的信息。
这种简化降低了建模的复杂度与模型的学习成本，
使得在大规模数据上学习词向量成为可能。
诸如Word2vec\cite{DBLP:journals/corr/MikolovSCCD13}，GloVe\cite{pennington-socher-manning:2014:EMNLP2014}等
一系列``静态''的词的向量化算法就是这种简化的最佳实践。
然而，这种简化也忽略了一个词在不同上下文环境下的句法语义差异。
举例来讲，``\textit{制服}''在``他\textit{制服}了窃贼''与``身穿该厂\textit{制服}的工人''
两种上下文环境下发挥的句法语义功能完全不同。\cite{guo-EtAl:2014:Coling}
给予不同环境中的两个词
以相同的表示做法
无疑是值得商榷的。

上下文相关词向量\cite{NIPS2017_7209,melamud-goldberger-dagan:2016:CoNLL,peters-EtAl:2018:N18-1,DBLP:journals/corr/abs-1810-04805}作为“静态”词向量的一种改进，取消了
“一个词有唯一的词向量”的假设，
在不同的上下文环境下，赋予相同的词不同的词向量，进而建模了
一个词上下文环境相关的句法语义差异。
这种词的向量化的技术在2017年末被提出，
并在2018年引起广泛关注，
迅速成为自然语言处理领域的热点。
%因成功地建模了一个词在不同上下文中的语义，
%上下文相关词向量技术迅速成为2018年自然语言处理领域的热点。
在其兴起的过程中，
两项代表性工作扮演了重要的角色。
首先，Peters等人在2018年的文献中\inlinecite{peters-EtAl:2018:N18-1}
提出的使用\textit{双向长短时记忆循环神经网络}（bidirectional long short-term memory network，
简称BiLSTM，文献\inlinecite{Hochreiter:1997:LSM:1246443.1246450}）
建模上下文的词向量的模型 --- Embeddings from Language Modeling（简称ELMo）。
在包括问答、文本蕴含、语义角色标注\cite{peters-EtAl:2018:N18-1}、
共指消解\cite{lee-he-zettlemoyer:2018:N18-2}、
短语结构句法\cite{joshi-peters-hopkins:2018:Long}等句法语义任务上取得很好的效果。
在文章发表时，
ELMo在6项自然语言处理任务中取得了当时最好的结果。
在ELMo发表后不久，
Devlin等人在2018年的文献\inlinecite{DBLP:journals/corr/abs-1810-04805}中
提出使用双向自注意力网络\footnote{self-attention network，又称Transformer\cite{NIPS2017_7181}。}（Bidirectional Encoder Representations from Transformers，简称Bert）
建模上下文，
同时用词级别与句子级别的\textit{完形填空}作为学习目标学习模型，
从而获得了
更好的上下文相关词向量。
在11项任务中，
Bert超越了ELMo，取得了当前最好的结果。
%上述结果都表明，上下文相关词向量是提高自然语言处理性能的一种有效途径。

包括分词\cite{Xue:2003:CWS:1119250.1119278,zhang-clark:2007:ACLMain,zheng-chen-xu:2013:EMNLP}、
词性标注\cite{DBLP:journals/corr/HuangXY15,ma-hovy:2016:P16-1}、
句法分析\cite{mcdonald2006online,nivre2008algorithms,zhang-clark:2008:ACLMain,chen-manning:2014:EMNLP2014,DBLP:journals/corr/DozatM16}
在内语言分析问题是自然语言处理的基石。
语言分析的性能很大程度上影响了后续任务的性能。
现阶段，基于统计学习的语言分析算法已经成为主流，
而基于神经网络的方法更是应用于自然语言处理中的一种重要的统计学习手段。
得益于词的成功表示，使用``静态''词向量的作为输入的
语言分析模型已经在相应任务上取得了很大的成功。\cite{zheng-chen-xu:2013:EMNLP,chen-manning:2014:EMNLP2014,DBLP:journals/corr/HuangXY15,ma-hovy:2016:P16-1,DBLP:journals/corr/DozatM16}
然而，在语言分析模型中使用``动态''的上下文相关的词向量尚未获得充分研究。
上下文相关词向量与语言分析技术的结合
中有诸多有趣并值得探索的问题。

\subsection{课题意义}[Significance]

自然语言是人与机器交互的一种重要手段；同时也是机器算法
感知分析人类社会的一种重要途径。
处理自然语言是实现机器智能的重要一步。
而语言分析作为其他自然语言任务的第一步，
其准去率、效率等对于处理自然语言有很大的影响。
同时，
大部分语言分析任务衍生于语言学问题，
研究语言分析问题可以帮助我们更好地人类语言。

时至今日，
统计学习已经成为语言分析乃至整个自然语言处理的主流方法。
影响统计学习算法效果的一个重要的因素是
对算法的输入进行合理的表示。
可以说，
基于统计学习的语言分析算法的发展是与表示方法的发展紧密相关的。
从早期使用统计量的表示方法\cite{Brown:1992:CNG:176313.176316,Brown:1993:MSM:972470.972474,Vogel:1996:HWA:993268.993313,eisner-1996-coling}，
到大规模离散特征\cite{collins:2002:EMNLP02,mcdonald2006online,daume05search,zhang-clark:cl:2011}，
再到现今流行的使用
词向量作为表示的方法\cite{DBLP:journals/corr/MikolovSCCD13,DBLP:journals/corr/HuangXY15,kim-rush:2016:EMNLP2016}，
语言分析模型也从适应离散特征线性模型\cite{NIPS2003_2397,Crammer:2006:OPA:1248547.1248566}
发展到
适应向量化表示的非线性模型\cite{hubel:monkey,Hochreiter:1997:LSM:1246443.1246450,Collobert:2011:NLP:1953048.2078186}。
而作为现今广泛应用的词的表示方法，
词向量的质量很大程度上影响了基于神经网络的语言分析算法
的性能。\cite{faruqui-EtAl:2016:RepEval,7478417,schnabel-EtAl:2015:EMNLP}
成功建模句法语义信息的词向量能更好地帮助
自然语言处理模型取得更好的准确率。
作为一种新兴的词表示技术，
上下文相关词向量
已经在一系列自然语言处理任务中表现出
巨大的潜力。
其对于基础自然语言任务
的作用非常值得研究。

以语言分析任务为场景
研究上下文相关词向量的意义是多方面的。
其中最主要的意义是
通过研究上下文相关词向量
\textit{提高语言分析模型的准确率}。
这种准确率的提升即包括
研究\textit{如何将上下文相关词向量应用在
语言分析任务中}，
也包括\textit{为语言分析任务
设计更有效的上下文相关词向量}。
其次，由于大部分语言分析任务衍生于语言学问题，
在语言分析任务中研究上下文相关词向量
可以\textit{分析并理解上下文相关词向量的性质}，
进而指导其在其他任务中的应用。
最后，
将使用上下文相关词向量的语言分析算法
部署到实际应用时
除了要关心性能，还要平和运行效率、资源开销等方面的问题。
从运行效率、资源开销的角度
研究上下文相关词向量可以\textit{使上下文相关
词向量在实际场景中得到更广泛的应用}。

从上面的研究意义出发，
本文拟围绕上下文相关词向量与语言分析任务
开展一系列研究。
为了使用上下文相关词向量\textit{提高语言分析的性能}，
本文尝试提出一种适应语言分析任务
的上下文相关词向量算法；
将上下文相关词向量与现有系统融合以提高这些系统的性能；
并探索能否基于上下文相关词向量构造更高层次的自然语言结构（片段、树等）
的表示。
为了\textit{分析并理解上下文相关词向量的性质}，
本文在大规模语言分析任务上
应用并分析上下文相关词向量的影响。
为了\textit{使上下文相关词向量在实际场景中得到更广泛的应用}，
本文尝试提出一种模型压缩的方法以实现又快又好的
语言分析的目标。

\section{研究现状与分析}[Related Work]

以语言分析任务为场景
研究上下文相关词向量包含两方面研究主体，
即：上下文相关词向量和语言分析任务。
在接下来的章节，
本文将首先围绕这两个研究主体，
首先
回顾上下文相关词向量的前身``上下文无关词向量''，
将其与上下文相关词向量建立广义的联系；
然后
分析现阶段主流的上下文相关词向量
的算法与性能；
最后
分析现阶段语言分析任务
的发展情况，
及其与上下文相关词向量潜在的结合方向。

\subsection{上下文无关词向量算法}[Static Word Vectorization Methods]
如前文所述，词的向量化表示是使用统计学习解决自然语言处理的基础。
广义上讲，所有将词转化为向量的算法都可以归纳为词的向量化算法。
这些算法中包括了传统``特征工程''的方法，即根据一系列专家定义的
启发式规则将词（以及其形态学、上下文等）特征转换为高维、稀疏、
并且通常是离散的向量。
本文关注的词的向量化的算法特指
将词转化为低维、稠密、连续的向量的算法。

在讨论使用低维向量表示词之前，
一个需要讨论的问题是：\textit{为什么词能够通过低维、稠密、连续的向量
进行表示？}
这一问题从一定程度上讲，可以
通过特征工程中``降维''思想进行回答。
``降维''可以将高维、稀疏、离散的具有可解释性的特征向量
近似表示为低维、稠密、连续的向量。
从实际应用的角度，这种降维后的向量往往具有不输于离散表示的性能，
甚至在一些任务中取得了更好的性能。\cite{lei-EtAl:2014:P14-1,lei-EtAl:2015:NAACL-HLT}
这一系列工作经验性地证明了词具有低维、稠密、连续的向量表示。
基于上述论述，本文假设词具有低维、稠密、连续的向量表示，即词向量。

为了使词向量能够在实际应用中发挥作用，
普遍接受的观点是词向量应该携带词的句法语义信息。\cite{DBLP:journals/corr/MikolovSCCD13,faruqui-EtAl:2016:RepEval,7478417}
这种句法语义信息通常是通过建模词义的分布式假设来实现的。
接下来，本文将从\textit{建模词义分布式假设}的角度出发，
分析一系列上下文无关的词向量算法。

\subsubsection{使用``共现矩阵''建模分布式假设的方法}[Methods that Inspired by Concurrence Matrix]\label{sec:intro:matrix}
\begin{figure}[t]
	\[
	\begin{bmatrix}
	 & \text{barn} & \text{fell} & \text{horse} & \text{past} & \text{raced} & \text{the} \\
	\text{barn} & 1 & 1 & 0  & 0 & 0 & 1 \\
	\text{fell} & 1 & 1 & 0  & 0 & 0 & 0 \\
	\text{horse} & 0 & 0 & 1  & 0 & 1 & 1 \\
	\text{past} & 0 & 0 & 0  & 1 & 1 & 1 \\
	\text{raced} & 0 & 0 & 1 & 1 & 1 & 0 \\
	\text{the} & 1 & 1 & 0  & 1 & 0 & 2 \\
	\end{bmatrix}
	\]
	\bicaption{}{使用词的上下文窗口构造共现矩阵的一个例子\cite{lund1996producing}。
	这个例子为``the horse raced past the barn fell''建立了窗口大小为2的共现矩阵。}{Fig.$\!$}{An example of the concurrence matrix which models the sentence ``the horse raced past the barn fell''
	with a window of 2 words.}
	\label{fig:1:example}
\end{figure}
词义分布式假设强调``上下文相似的词有相似的词义''。
词义分布式假设包含两部分主体：
一个是\textit{上下文}，
另一个是被描述的词（下文称``\textit{目标词}''）。
建模词义分布式假设本质上是：
1）选择一种方式描述上下文；
2）选择一种模型描述目标词与其上下文之间的关系。

最简单的描述上下文并描述目标词与上下文关系的方法是
关注目标词与哪些词或文档共现。
潜在语义分析（Latent semantic analysis，简称LSA）是这类方法的代表。
最早，LSA提出的目标是要建模信息检索中词义相似的词。
LSA中的一个目标词首先被表示为一个向量。
向量各维表示这个词在各个文档中出现的计量\footnote{这种计量可以是频次，tf-idf，点互信息等。}。
然后，所有词被堆叠成为一个矩阵，即\textit{共现矩阵}。
最后经过矩阵分解技术，
原始的词向量被映射到低维的表达词的潜在语义的空间。
高维到低维之间存在线性变换是一种常用的降维的假设。
基于这种假设的降维方式包括：奇异值分解（Singular Value Decomposition，简称SVD）等。

从建模分布式假设的角度来看，
LSA使用目标词所在的文档描述目标词的上下文。
除了使用文档间接地描述上下文，
目标词周围的词也可以描述上下文，从而构造共现矩阵。
Lund与Curt在1996年的文献\inlinecite{lund1996producing}中
提出Hyperspace Analogue to Language（简称HAL）算法，
首次使用一定窗口内词的共现次数建模目标词的词义
（如图\ref{fig:1:example}所示）。
Rohde等人在2006年的文献\inlinecite{Rohde06animproved}中
提出COALS算法并对HAL进行了一系列改进。
这些改进包括：将共现矩阵中的计数转化为皮尔逊相关系数（Pearson's correlation）
并去掉负相关的共现，同时使用SVD分解获得词的低维的向量化表示。
文献\inlinecite{Rohde06animproved}中的工作可以认为是使用低维向量化的形式表示词的先驱工作。
在这之后，多种基于共现矩阵建模词义分布式假设的词向量算法
被提出。其中包括文献\inlinecite{lebret-collobert:2014:EACL}，
文献\inlinecite{pennington-socher-manning:2014:EMNLP2014}等。
值得一提的是，
Pennington等人在2014年的文献\inlinecite{pennington-socher-manning:2014:EMNLP2014}中
提出的GloVe算法也使用共现矩阵建模了词义分布式假设。
但GloVe算法并没有采用矩阵分解的思路进行降维，
而是
首先假设目标词存在低维向量化表示，
然后将向量化的表示输入一个函数并用回归的方式对共现矩阵
中的值进行拟合以达到降维（或向量化）的效果。

Word2vec\cite{DBLP:journals/corr/MikolovSCCD13}是另一个流行的向量化算法。
在提出之初，
Word2vec算法可以概括为将目标词的向量化的表示输入一个神经网络
预测其在生语料中的上下文。
可以说，Word2vec是使用``预测目标词或上下文''的思路建模分布式假设。
但Levy等人在2014年的文献\inlinecite{NIPS2014_5477}中证明在一定条件下，
Word2vec等价于对于目标词及其窗口内上下文的点互信息矩阵进行SVD分解。
这一发现将基于``共现矩阵''与基于``预测目标词或上下文''的方法进行了统一。
后文会从``预测目标词或上下文''的角度对Word2vec进行讨论。

\subsubsection{使用``预测目标词''建模分布式假设的方法}\label{sec:intro:predict}
除了通过目标词与上下文的共现描述两者关系，
通过上下文$c = \{x_i\}$
预测目标词$y$
的出现也是一种描述两者关系
的常用方法\footnote{
	在第\ref{sec:intro:matrix}节中，``目标词''代表待向量化的词。
	但在第\ref{sec:intro:predict}中，``目标词''与``上下文''在向量化的语义下没有实质差别，
	故这一节的``目标词''特指要预测的词。}。
这种方法
的第一步是定义一个词到其向量的映射关系$\phi: \mathbb{R}^{d} \leftarrow \mathcal{X}$\footnote{即词向量表。}。
将上下文的词$x_i \in c$输入到$\phi$中
获得他的向量表示$\mathbf{v}_i = \phi(x_i)$，
并把这些向量输入到一个建模上下文的函数$\mathcal{F}_{c} (\{\mathbf{v}_i\})$，
从而获得其上下文表示$\mathbf{c}$，
最后将$\mathbf{c}$输入到一个预测函数$\mathcal{F}_{p} (\mathbf{c}, y)$
达到描述两者关系的目标。
在这类方法中，
$\phi$是词向量算法的最终产出。

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth, trim={0 0 12.2cm 11cm},clip]{intro/bengio03}
	\bicaption{}{Bengio等人提出的语言模型。}{Fig.$\!$}{The language model proposed by Bengio et al.}
	\label{fig:intro:bengio03}
\end{figure}

\paragraph{基于语言模型的方法}
前人工作探索了不同的上下文函数$\mathcal{F}_{c}$与不同预测函数$\mathcal{F}_{p}$。
一种典型的组合是使用前文表示上下文，根据前文预测下一个词，
即\textit{语言模型}。
Bengio等人在2003年的文献\inlinecite{NIPS2000_1839}中提出
一种利用有限窗口上文预测下一个词的语言模型（如图\ref{fig:intro:bengio03}所示），
成为这一类方法的先驱。
具体来讲，给定一个窗口大小为$m$的上文$x_{t-m}, ..., x_{t}$以及目标词$x_{t+1}$，
文献\inlinecite{NIPS2000_1839}使用向量拼接的方式建模$\mathcal{F}_{c}$，
并使用多层神经网络建模用上文预测下一个词的概率
$\mathcal{F}_{p} = p(x_{t+1} \mid x_{t-m}, ..., x_{t})$。
其模型可以形式化地描述为，
\begin{align*}
\mathbf{c} &= \oplus_{i=t-m}^{t} \mathbf{v}_i \\
\mathbf{h} &= \text{tanh}(\mathbf{W}^{(1)} \mathbf{c} + \mathbf{b}^{(1)}) \\
\mathbf{s} &= \mathbf{W}^{(3)} \mathbf{c} + \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(3)}\\
p(x_{t+1} \mid x_{t-m}, ..., x_t) &= \frac{1}{Z} \exp(\mathbf{s}_{x_{t+1}})\text{。}
\end{align*}
其中，$\oplus$代表向量的顺序拼接，
归一化因子$Z = \sum_{y'} \exp(\mathbf{s}_{y'})$\footnote{后文将采用此记号代表所有softmax函数的归一化因子。}。
后续的工作在维持建模方式不变的前提下
对Bengio的模型进行了一系列改进，
其中包括：
Mnih与Hinton在2007年的文献\inlinecite{Mnih:2007:TNG:1273496.1273577}
中提出的使用加权求和
取代拼接以建模上下文$\mathcal{F}_{c}$
的Log-bilinear Language Model（简称LBL），
以及
Mnih与Hinton在2007年的文献\inlinecite{NIPS2008_3583}中
提出的
使用基于词表聚类的启发式负采样
提高LBL中预测函数$\mathcal{F}_{p}$
的学习速度的
Hierarchical Log-Bilinear Language Model。

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth, trim={0 0 12.2cm 12.5cm},clip]{intro/rnnlm}
	\bicaption{}{基于循环神经网络的语言模型。}{Fig.$\!$}{Recurrent Neural Network Language Model.}\label{fig:intro:rnnlm}
\end{figure}
除了使用简单拼接或者加权平均的方法建模上下文$\mathcal{F}_{c}$，
循环神经网络（recurrent neural network，简称RNN）
具备建模任意长度历史的能力，
因而也可以作为
上下文的一种建模。
Mikolov等人在2010年的文献\inlinecite{DBLP:conf/interspeech/MikolovKBCK10}
中提出RNN语言模型建模词的上下文（如图\ref{fig:intro:rnnlm}所示）。
与文献\inlinecite{NIPS2000_1839}的思路一样，RNN语言模型
也尝试建模一定上文条件下一个词出现的概率。
具体来讲，其形式如下：
\begin{align*}
\mathbf{c} &= \text{RNN}(\mathbf{v}_1, ..., \mathbf{v}_t) \\
\mathbf{s} &= \mathbf{W} \mathbf{c}  + \mathbf{b} \\
p(x_{t+1} \mid x_{t-m}, ..., x_t) &= \frac{1}{Z} \exp(\mathbf{s}_{x_{t+1}})\text{。}
\end{align*}

可以说，
语言模型是最早的成功描述上下文以及上下文与目标词关系的方法。
通过分析可见，
语言模型的思想在
在上下文相关词向量中也得到了应用。

\paragraph{基于完形填空的方法}
Collobert等人在2011年的文献\inlinecite{Collobert:2011:NLP:1953048.2078186}
中提出了一种另一种描述上下文以及上下文与目标词关系的方法，
即
在挖去目标词的一个上下文环境中出现目标词的可能性应大于出现其他词。
具体来讲，
对于窗口为$2m$的上下文，
文献\inlinecite{Collobert:2011:NLP:1953048.2078186}
使用卷积神经网（convolutional neural network，简称CNN，文献\inlinecite{hubel:monkey}）
建模上下文$\mathcal{F}_{c}$，
并将上下文向量输入一个前馈神经网络中获得一个标量$s$作为这一上下文``真实性''的一种度量。
具体来讲，其模型可以描述为
\begin{align*}
\mathbf{c} &= \text{CNN}(\mathbf{v}_{t-m},..., \mathbf{v}_{t+m}) \\
\mathbf{h} & = \text{tanh}(\mathbf{W}^{(1)}  \mathbf{c} + \mathbf{b}^{(1)}) \\
s &= \mathbf{W}^{(2)} \mathbf{h} +\textbf{b}^{(2)} \text{。}
\end{align*}
为了训练模型参数，文献\inlinecite{Collobert:2011:NLP:1953048.2078186}提出
一种基于完形填空的训练思想，即真实的上下文的得分应高于将目标词随机替换后获得的上下文。
这种训练思想被使用最大化分类边界学习目标（margin loss）建模。

Collobert等人提出的完形填空的思想有诸多独到之处。
其中相对语言模型来讲，
其最为重要的不同是在描述上下文时不只考虑上文，更可以考虑下文。
可以说，这种思想与Devlin等人提出的Bert上下文相关词向量有紧密的联系。

\paragraph{基于单个上下文词与目标词关系的方法}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\columnwidth, trim={0 0 11cm 9cm},clip]{intro/word2vec}
	\bicaption{}{文献\inlinecite{DBLP:journals/corr/MikolovSCCD13}提出的两种模型：CBOW（左）与Skip-gram（右）。}{Fig.$\!$}{The CBOW model (left) and Skip-gram model (right) from Mikolov et al. (2013)\cite{DBLP:journals/corr/MikolovSCCD13}}\label{fig:intro:word2vec}
\end{figure}

如前文所述，建模词义分布式假设的核心思想由两部分组成：
1）选择一种方式描述上下文；
2）选择一种模型刻画目标词与其上下文之间的关系。
使用语言模型以及使用完形填空的方法
都对一定宽度内上下文整体进行了表示。
能否将上下文的整体表示化简为单个独立的词
并预测单个上下文词与目标词之间的关系呢？
Mikolov等人在2013年文献\inlinecite{DBLP:journals/corr/MikolovSCCD13}中
基于这种思想
提出了Word2vec模型。


相较语言模型，Word2vec模型采用某个上下文词的词向量$\mathbf{v}'$作为
$\mathcal{F}_c$，
并使用
目标词预测这个上下文或这个上下文预测目标词的概率作为$\mathcal{F}_p$。
文献\inlinecite{DBLP:journals/corr/MikolovSCCD13}提出两种
模型建模，分别是CBOW与Skip-gram模型（参考图\ref{fig:intro:word2vec}）。
CBOW建模的是根据上下文词的评价预测目标词的概率，即$p(x_{t} \mid x_{t-m}, ..., x_{t-1}, x_{t+1}, ..., x_{t+m})$；
Skip-gram建模的是根据目标词预测某个上下文的概率，即$p(x_i | x_t)$，其中$t-n\le i\le t+n, i \ne t$。
具体来讲，CBOW模型可以描述为
\begin{align*}
\mathbf{c} &= \frac{1}{2n}\sum_{t-n\le i\le t+n, i \ne t } \mathbf{v}_i \\
p(x_{t} \mid x_{t-n: t-1}, x_{t+1: t+n}) & = \frac{1}{Z} \exp (\mathbf{v}_t^{T} \cdot \mathbf{c})\text{。}
\end{align*}
而Skip-gram可以描述为
\[
p(x_i \mid x_t) = \frac{1}{Z} \exp (\mathbf{v}_t^T \cdot \mathbf{v}'_i)\text{。}
\]
需要说明的是，文献\inlinecite{DBLP:journals/corr/MikolovSCCD13}
采用两套不同的词向量分别建模了上下文词向量与预测词向量。

Word2vec算法作为基于神经网络的自然语言处理模型的基础
对于自然语言处理的影响是深远的。
但Word2vec对于上下文以及外部知识的过度简化也激发了
一系列工作。
由于Word2vec没有考虑目标词与上下文词的相对位置，
Wang等人在文献\inlinecite{ling-EtAl:2015:NAACL-HLT}中
提出使用不同的$\phi$表示不同相对位置的词，从而改进了原始算法。
由于Word2vec在建模词义时没有考虑人工构建的词义词典，
一系列工作也尝试将
外部知识，比如WordNet\cite{rothe-schutze:2015:ACL-IJCNLP}，
双语词典\cite{DBLP:journals/corr/SmithTHH17}，
平行语料\cite{AAAI1612236}，
远程监督信息\cite{tang-EtAl:2014:P14-1}等，
引入Word2vec的模型学习中。

\paragraph{上下文无关词向量的评价}
至此，本文已经对当前具有代表性的``静态''词的向量化算法进行了回顾。
一个自然产生的问题是比较这些向量化的算法的优劣。
Lai等人在2016年的文献\inlinecite{7478417}中对这些``静态''词向量化算法从多个方面进行了比较。
文献\inlinecite{7478417}比较的内容包括不同的因素对于产生的词向量的质量的影响。
文献\inlinecite{7478417}采用语义相似度、语义类比，用作特征时的模型性能，用作初始化时的模型性能
三类指标评价了不同词向量的性能。
这一系列工作尝试回答：1）训练语料规模对于模型性能的影响；
2）相同的参数设置下，哪种模型性能更好。
对于第一个问题，文献\inlinecite{7478417}的结论是
是训练数据规模越大，得到的词向量性能越好。
对于第二个问题，他们的研究没有给出明确的答案。
不同的词向量算法在不同的实验指标下
表现并不一致。
Yin与Schutze也在2016年的文献\inlinecite{yin-schutze:2016:P16-1}
中验证了这一观察。

\begin{table}[t]
\centering
\bicaption{}{前人工作中中提到的不同词向量的性能。}{Table $\!$}{A comparison on static word embeddings.}\label{tbl:1:emb-on-task}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{r | p{4.3cm}p{2.8cm} | cc | c}
	\toprule[1.5pt]
	 & \multirow{2}{*}{上下文表示} &  \multirow{2}{2.5cm}{目标词与上下文的关系} & \multicolumn{2}{c|}{文献\inlinecite{7478417}} & 文献\inlinecite{yin-schutze:2016:P16-1} \\
	 &  &  & NER & POS & POS \\
	\midrule[1pt]
	Random & & & 84.39 & 95.41 & - \\
	\hdashline
	GloVe & 加权的共现比例 & 预测共现比例 & 88.19 & 96.42 & 96.65 \\
	Skip-gram & 上下文某个词的词向量 & 预测目标词& \textbf{88.90} & 96.57 & - \\
	CBOW & 上下文各词词向量的平均值& 预测目标词& 88.47 & 96.63 & 96.40 \\
	LBL & 上下文各词的语义组合& 预测目标词& 88.69 & \textbf{96.77} & - \\
	HLBL& 上下文各词的语义组合& 预测目标词& - & - & 96.53 \\
	NNLM & 上下文各词的语义组合& 预测目标词& 88.36 & 96.73 & - \\
	C\&W & 上下文各词与目标词的语义组合& 上下文与目标词的联合打分& 88.15 & 96.66 & 96.58 \\
	\bottomrule[1.5pt]
\end{tabular}
\end{table}
值得一提的是，词向量化的评价是一个争议很大的问题。
词义相似度是很多词向量算法使用的评价指标。
然而，Faruqui等人在文献\inlinecite{faruqui-EtAl:2016:RepEval}中
指出词义相似度有诸多问题，比如：词义相似度的评价具有主观性，
与特定任务关联较强，与下游任务的性能相关性较弱，
不能评价一词多义的现象等。

本文关注的主要问题是词向量与下游句法任务的关系，
一般来讲，
词向量在神经网络中的应用一般有两种模式：
1）\textit{用作特征}，
以及2）\textit{用作神经网络的初始化}。
用作特征意味着词向量用作输入，
并在模型学习中保持不变。
用作初始化意味着词向量用来初始化参数。
用作特征的方式使得
模型可以给未出现在训练数据中的词（指的是特定任务的训练数据）
一个合理的词向量。
这种方式往往给模型带来更好的泛化性，
但由于词向量的也给模型学习带来了困难。
用作初始化的模型往往更容易学习，但也牺牲了一定的泛化性。
两种方式孰优孰劣无法从理论上给出解释，
大部分工作是依靠实际任务的性能进行评价的。

关于不同的词向量在实际任务中的表现一直众说纷纭。
文献\inlinecite{7478417}与文献\inlinecite{yin-schutze:2016:P16-1}
都讨论了这一问题。
表\ref{tbl:1:emb-on-task}总结了他们的比较结果。
两篇文献都在相同的数据规模的设置下重新训练了词向量。
其中，文献\inlinecite{7478417}在CoNLL03命名实体识别数据集\cite{TjongKimSang-DeMeulder:2003:CONLL}上将
词向量用作特征，在Penn Treebank （简称PTB）词性标注数据集\cite{Marcus93buildinga}上将词向量用作初始化。
文献\inlinecite{yin-schutze:2016:P16-1}在PTB词性标注数据集
上将词向量用作初始化。
从表\ref{tbl:1:emb-on-task}可以看出，不同的词向量的性能存在一定差异，但这种差异并不显著，
同时某种词向量并不稳定地好于其他词向量。
究其原因，本文认为主要的问题在于
这类方法采用静态的词向量表示方式，即一个词只有唯一的向量化表示。
这种表示方式使得词向量无法显示地建模一个词在不同上下文环境下
句法语义功能的不同。

\subsection{上下文相关词向量算法}[Contextualized Word Vectorization Methods]
一个词在不同的上下文情境下发挥的句法语义功能可能存在很大差异。
举例来讲，``制服''在``他\textit{制服}了窃贼''与``身穿该厂\textit{制服}的工人''
两种上下文环境下分别用作动词（代表``用强力使之驯服''）以及名词（代表``有规定式样的服装''）。
给两种上下文环境下的``制服''相同的向量表示的做法无疑是值得怀疑的。

Melamud等人在2016年的文献\inlinecite{melamud-goldberger-dagan:2016:CoNLL}
中首次从词向量的角度讨论这一问题
并提出context2vec算法。
context2vec在描述预测目标词与上下文关系时
可以类比CBOW模型。
但不同的是，context2vec采用LSTM描述
上下文$\mathcal{F}_c$。
具体来讲，context2vec将
从左向右以及从右向左的词
分别输入两个LSTM中，
并将隐层输出作为上下文的表示。
context2vec可以形式化地定义为：
\begin{align*}
\hat{\mathbf{c}}_t &= \overrightarrow{\text{LSTM}}(\mathbf{v}_{1}, ..., \mathbf{v}_{t-1}) \oplus \overleftarrow{\text{LSTM}}(\mathbf{v}_{t+1}, ..., \mathbf{v}_{n}) \\
\mathbf{h}_t & = \text{ReLU}(\mathbf{W}^{(1)} \hat{\mathbf{c}}_t  + \mathbf{b}^{(1)}) \\
\mathbf{c}_t &= \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)} \\
p(x_t \mid x_1, ..., x_{t-1}, x_{t+1}, ..., x_{n}) & = \frac{1}{Z} \exp(\mathbf{v_t}^T \cdot \mathbf{c}_t)\text{。}
\end{align*}
除了选择不同的方式描述上下文，
相对Word2vec一类的上下文无关词向量，
context2vec最重要的不同在于\textbf{采用$\mathbf{c}_t$
作为$x_t$的词向量}。
这种做法使得同一个词在不同的上下文条件下拥有不同的向量表示。
可以说，context2vec是上下文相关词向量的最早尝试。

McCann等人在2017年的文献\inlinecite{NIPS2017_7209}中
再次强调了依照上下文对词进行向量化的重要性。
在文献\inlinecite{NIPS2017_7209}中，
McCann等人
使用神经网络的机器翻译\cite{NIPS2014_5346,luong-pham-manning:2015:EMNLP,DBLP:journals/corr/BahdanauCB14}的
解码器的输出作为对应词的上下文相关词向量，
并将其算法命名为上下文相关词向量（Contextualized word vectors，简称CoVe）。
虽然并未明确定义上下文与目标词，
可以认为CoVe使用双向LSTM描述上下文$\mathcal{F}_c$，
并利用机器翻译中目标语的词作为目标词，
从而描述了源语言上下文与目标语词之间的关系。
McCann等人将CoVe词向量作为特征输入，
并在情感分析\cite{socher-EtAl:2013:ACL2013}、
问题分类\cite{DBLP:conf/trec/Voorhees99}、
文本蕴含\cite{bowman-EtAl:2015:EMNLP}
以及阅读理解\cite{rajpurkar-EtAl:2016:EMNLP2016}
中取得了性能的提升。

从建模词义分布式假设的角度讲，
CoVe并未对目标词以及上下的关系进行明确定义。
2018年，Peters等人在文献\inlinecite{peters-EtAl:2018:N18-1}中
明确将目标词定义为语言模型中待预测的下一个词，
并提出使用双向语言模型中
隐层作为上下文表示的模型 --- ELMo。
具体来讲，ELMo可以形式化地定义为：
\[
\mathbf{c}_t = \overrightarrow{\text{LSTM}}(\mathbf{v}_{1}, ..., \mathbf{v}_{t}) \oplus \overleftarrow{\text{LSTM}}(\mathbf{v}_{t}, ..., \mathbf{v}_{n})\text{。}
\]
其中$\mathbf{c}_t$代表$x_t$的上下文相关词向量。
而$\overrightarrow{\text{LSTM}}(\mathbf{v}_{1}, ..., \mathbf{v}_{t})$与$\overleftarrow{\text{LSTM}}(\mathbf{v}_{t}, ..., \mathbf{v}_{n})$
是通过分别最大化前向与后向语言模型学习目标：
\begin{align*}
\overrightarrow{\mathbf{c}}_t &=\overrightarrow{\text{LSTM}}(\mathbf{v}_{1}, ..., \mathbf{v}_{t})\\
\mathbf{\Theta} &= \argmax_{\mathbf{\Theta}} p(y = x_{t+1} \mid x_{1}, ..., x_{t} ) \\
 & = \argmax_{\mathbf{\Theta}} \frac{1}{Z}\exp( \mathbf{W}_{x_{t}} \overrightarrow{\mathbf{c}}_t + \mathbf{b}_{x_{t}})
\end{align*}
学习模型参数$\mathbf{\Theta}$的\footnote{这里只举前向为例，后向$\overleftarrow{\text{LSTM}}$的参数学习方法相同。}。
文献\inlinecite{peters-EtAl:2018:N18-1}
使用2层双向LSTM描述上下文$\mathcal{F}_c$，
并用2层LSTM的隐层输出的加权求和作为
最终的上下文相关词向量。形式化地可以表示为：
\[
\mathbf{c}_t = \gamma \sum_{j=0}^2 s^j \cdot (\overrightarrow{\text{LSTM}}(\mathbf{v}_{1}, ..., \mathbf{v}_{t}) \oplus \overleftarrow{\text{LSTM}}(\mathbf{v}_{t}, ..., \mathbf{v}_{n}))\text{。}
\]
文献\inlinecite{peters-EtAl:2018:N18-1}将上下文相关词向量分别用作特征，
并在包括阅读理解\cite{rajpurkar-EtAl:2016:EMNLP2016}、
文本蕴含\cite{bowman-EtAl:2015:EMNLP}、
语义角色标注\yjcomment{citation}、
共指消解\yjcomment{citation}、
命名实体识别\cite{TjongKimSang-DeMeulder:2003:CONLL}、
以及情感分析\cite{socher-EtAl:2013:ACL2013}
在内的6项数据集上取得了当前最优结果。

除了性能的提升，文献\inlinecite{peters-EtAl:2018:N18-1}也指出
对于多层LSTM，使用语言模型为目标学习出的网络具有
从句法到语义的逐步抽象能力。
具体来讲，LSTM的底层有更好的句法抽象能力，高层具有
更好的语义抽象能力\footnote{实践中，底层在词性标注等典型句法任务中表现更好，高层在共指消解等典型语义任务中表现更好。}。
同年，Peters等人在文献\inlinecite{peters-EtAl:2018:EMNLP}中
对于ELMo进行了更加深入的分析。
根据文献\inlinecite{peters-EtAl:2018:EMNLP}，
ELMo的成功主要来自从大规模生语料中使用语言模型的
学习目标学习参数。
采用哪种网络\footnote{文献\inlinecite{peters-EtAl:2018:EMNLP}尝试了Gated CNN \cite{pmlr-v70-dauphin17a}，
	Transformer \cite{NIPS2017_7181}两种网络结构。}
表示上下文对于最终模型性能的影响并不大。
同时，不同的网络都表现出底层的句法抽象能力更好，
高层的语义抽象能力更好的特性。

可以看出，从描述上下文都描述上下文与目标词的关系，
ELMo都可以视作是基于RNN的语言模型的``上下文相关''版。
不同的是，前人工作将RNN语言模型中的词向量表视作
向量化的最终产物。
而ELMo则将整个RNN视作是向量化的产物并取得了巨大的成功。
这一观念及用法的改变源自近年来
语言模型技术的发展\yjcomment{cite}以及
多任务学习技术的发展\yjcomment{cite}。
沿着语言模型的思路，
Radford等人在2018年的文献\yjcomment{citation}中提出使用
单向Transformer建模上文的模型 --- Generative Pre-Training（简称GPT）。
除了描述上下方式的不同，
GPT的最大特点是通过语言模型预训练并初始化GPT，
然后在特定任务上进行精细调参（fine-tune parameters）。
这种方法进一步提升了模型性能。

\begin{table}[t]
	\bicaption{}{上下文相关词向量的比较。}{Table$\!$}{A comparison on contextualized word embeddings}\label{tbl:intro:context-emb}
	\vspace{0.5em}\centering\wuhao
	\begin{tabular}{r lp{3.8cm}lc}
		\toprule[1.5pt]
		模型名称 &上下文表示 & 目标词与上下文的关系 & 用法 & Multi-NLI结果 \\
		\midrule[1pt]
		CoVe & 双向LSTM &  机器翻译& 特征 & - \\
		ELMo & 双向多层LSTM &  双向语言模型 & 特征 & 79.6/79.3 \\
		ELMo+ & 双向多层Gated CNN  & 双向语言模型 & 特征& 78.3/77.9 \\
		& 双向Transformer  & 双向语言模型 & 特征& 79.4/78.3 \\
		GPT & Transformer   & 单向语言模型 & 初始化& 81.8/- \\
		Bert & Transformer & 词级别完形填空，句子级完形填空 & 初始化 & 86.7/85.9
\\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}


2018年下半年，Devlin等人基于GPT预训练的思想，
在文献\inlinecite{DBLP:journals/corr/abs-1810-04805}中提出使用
双向自注意网络（Bidirectional Encoder Representations from Transformers，简称Bert）建模目标词的左侧和右侧的上下文。
通过将目标词遮罩
并使用
词级别完形填空（或称遮罩语言模型，masked language model）
和句子级完形填空的学习目标训练双向自注意网络，
从而取得了
11项任务的大幅度性能提升，获得了超越ELMo与GPT的效果。

相对ELMo与GPT，Bert对于建模词义分布式假设
做出的是多方面的。
从描述上下文的角度，
Bert利用双向自注意网络的特点，真正地描述了``双向''的上下文。
从描述上下文与目标词的角度，
Bert提出的两个学习目标如下：
\begin{itemize}
	\item 词级别完形填空：随机将句子中的词替换为一个特殊标记，并用
	这个位置的上下文相关词向量预测原来的词；
	\item 句子级完形填空：将两个句子连接起来，并预测第二个句子
	是不是第一个句子的后续句子。值得一提的是，这种句子级完形填空
	的分类问题是在词级别上进行预测与学习的。
\end{itemize}
这种方式与语言模型有很大差异。
但与Collobert等人2011年的工作\inlinecite{Collobert:2011:NLP:1953048.2078186}
有相似之处。

通过上述讨论不难看出，
虽然产生巨大变革，
上下文相关词向量仍可以
套用建模词义分布式假设的两个维度：
上下文以及上下文和目标词的关系
进行解释
表\ref{tbl:intro:context-emb}从
这两个维度对上述上下文相关词向量方法进行总结。
同时，
需要注意的是现今主要的上下文相关词向量算法
都可以在其上下文无关词向量算法中找到对应。
能否借鉴上下文无关词向量算法中建模词义分布式假设的方法，
以及如何将上下文无关词向量的一系列研究成果与
这些上下文相关的词向量算法进行结合
都是值得研究的问题。

\subsection{基于神经网络的语言分析技术}[Language Technique Based on Neural Network]
包括分词、词性标注、命名实体识别、句法分析
在内的语言分析任务是
下游自然语言处理任务
的基础。
语言分析模型的性能很大程度上影响了下游任务的准去率。
因而吸引了大量科研兴趣。

从结构预测的观点看待这些语言分析问题，
每种问题都有特殊的结构与之对应。
其中，分词、命名实体识别对应分割结构预测问题；
词性标注对应序列结构预测问题；
句法分析对应树结构预测问题。
在神经网络兴盛之前，
研究人员热衷于对自然语言结构进行复杂的建模。
这类研究趋势可以进行如下总结：
1）\textit{强调全局解码}：从最大熵马尔科夫模型\cite{McCallum:2000:MEM:645529.658277}
到条件随机场模型\cite{Lafferty:2001:CRF:645530.655813}
是这一研究范式的代表。
这类研究强调在全局条件下无偏地求最优结构（应用于测试阶段）
以及无偏地计算边缘概率（应用于训练阶段）。
2）\textit{强调高阶结构}：从句法分析的
一阶Eisner算法\cite{eisner-1996-coling}到
二阶甚至更高阶的最大生成树算法\cite{mcdonald2006online}
是这一研究范式的代表。
同时，
这种思想也激发了以牺牲精确解码换取任意阶特征
的非精确解码算法\cite{zhang-clark:cl:2011}。

全局解码与高阶特征在神经网络兴起之前
受到推崇的一大原因是稀疏的词汇化特征表达能力不足。
实际，早在2008年，Liang等人就在文献\inlinecite{liang:icml08}
中指出解码的复杂程度与表示复杂程度存在某种折中。
即简单的解码与复杂的特征表示可以达到复杂的解码与简单的特征表示
相同的性能。

Liang等人的观点在基于神经网络的语言分析中得到充分的验证。
基于神经网络的语言分析模型的一大特点就是
表示模型无一例外非常复杂。
当前性能最优的词性标注模型\cite{ma-hovy:2016:P16-1,lample-EtAl:2016:N16-1}
使用LSTM或CNN建模词，
并使用多层双向LSTM建模上下文。
性能最优的分词模型\cite{ma-ganchev-weiss:2018:EMNLP}
使用也双向LSTM建模上下文。
性能最优的依存句法分析器\cite{DBLP:journals/corr/DozatM16}也采用类似的上下文表示。
可以说，
双向LSTM已经成为诸多语言分析模型的标准配置。
伴随着LSTM的大规模应用，
全局解码与高阶特征或被抛弃或被证明对于性能影响较小。
Dozat与Manning在2016年的文献\inlinecite{DBLP:journals/corr/DozatM16}中
提出的当前性能最优的深度双仿射句法分析器
是这一最新科研范式的最佳例证。
深度双仿射句法分析器完全抛弃了
稀疏特征时代标配的Esiner算法（或更高阶的最大生成树算法），
将依存句法分析化简为分类问题。
相同的，
Ma等人在文献\inlinecite{ma-ganchev-weiss:2018:EMNLP}中
提出的
性能最优的中文分词器也只依赖于字级别的分类，
而抛弃了稀疏特征时代的``标配''条件随机场模型。

基于上述分析，
强化基于神经网络的语言分析模型中的表示模型
是提高语言分析性能的重要而有效的手段。
上下文相关词向量作为一种
有效的表示模型有望提高
语言分析模型的性能。

\section{本文的研究内容及章节安排}[Content and Outlines]

本文针对上下文相关词向量在语言分析技术
中的应用开展一系列研究工作。
这些工作包括：
1）为语言分析技术有针对性地设计上下文相关词向量；
2）从建模词汇的角度利用上下文相关词向量优化产生序列和树的语言分析模型，并分析其带来性能提升的原因；
3）从组合特性的角度利用上下文相关词向量优化产生分割的语言分析模型；
4）利用知识蒸馏提高上下文相关词向量强化的模型的运行效率，降低资源开销，提高其实用性。

具体来讲，本文包含5章。各章节内容组织如下：

在第\ref{chp:intro}章中，
本文介绍了本文课题的研究背景、意义，并对上下文相关词向量
及其基础自然语言处理方面的应用反面
的研究现状
进行概述与分析，
最后对本文主要内容进行了规划。

在第\ref{chp:elmo}章中，
由于现有上下文相关词向量训练开销巨大、
无法进行词级别并行等问题，
本文提出一种
针对语言分析技术的
基于
局部上下文的上下文相关词向量。
\yjcomment{取得了什么}

在第\ref{chp:seqlabel}章中，
由于上下文相关词向量对于
包括分词、词性标注、句法分析在内的
基础自然语言处理问题的作用
尚不清楚，
本文以CoNLL 2018多国语句法分析评测
为基础，
详细研究了上下文相关词向量
对于这些基础自然语言处理问题的作用。
并且分析了上下文相关词向量
带来性能提升的原因。
本文研究显示上下文相关词向量
通过更好地未登录词的抽象，
达到更好的准确率。

在第\ref{chp:semicrf}章中，
由于上下文相关词向量
能否建模片段，
本文研究了。

在第\ref{chp:distill}章中，
本文研究了通过知识蒸馏
技术奖使用上下文相关词向量的
复杂模型蒸馏为简单模型。

本文各章节之间逻辑关系如图\yjcomment{s}所示。