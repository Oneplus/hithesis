% !Mode:: "TeX:UTF-8"
% !TeX spellcheck = zh_CN 

\chapter{绪论}[Introduction]\label{chp:intro}

\section{课题背景及意义}[Background and Significance]

\subsection{课题背景}[Background]

\textbf{词向量}\cite{DBLP:journals/corr/abs-1301-3781,NIPS2013_5021,pennington-socher-manning:2014:EMNLP2014,Q17-1010}
是基于深度学习的自然语言处理的基础。
词向量为自然语言的最小语义单元 --- 词提供了包含句法语义信息的稠密向量表示。
通过使用词向量作为深度神经网络的输入，
一个词自身的属性（如：词性\cite{DBLP:journals/corr/HuangXY15}），
一段词之间的关系（如：命名实体\cite{lample-EtAl:2016:N16-1}），
两个词之间的关系（如：句法\cite{dyer-EtAl:2015:ACL-IJCNLP,DBLP:journals/corr/DozatM16}）
以及一串词组成的句子、篇章的语义主旨\cite{kim-rush:2016:EMNLP2016}
等都可以得到建模。
词向量对于深度学习在自然语言处理中的广泛应用起到了重要的作用。

向量化一个词的语言学基础是词义的分布式假设（distributional hypothesis，文献\inlinecite{firth57synopsis}），
即：\textbf{一个词的词义可以采用它的上下文进行表示}。
现今流行的词的向量化方法进一步对词向量进行了静态假设，
即：\textbf{一个词有唯一的向量表示}。
%这种唯一的向量表示抽象了一个词在不同上下文中的句法语义功能。
这种假设降低了建模的复杂度与模型的学习成本，
使得在大规模数据上学习词向量成为可能。
包括Word2vec\cite{DBLP:journals/corr/abs-1301-3781,NIPS2013_5021}，GloVe\cite{pennington-socher-manning:2014:EMNLP2014}以及FastText\cite{Q17-1010}在内的
一系列``静态''的词向量化算法就是这种简化的最佳实践。
然而，这种假设也忽略了一个词在不同上下文环境下的句法语义差异。
举例来讲，``\textit{制服}''在``他\textit{制服}了窃贼''与``身穿该厂\textit{制服}的工人''
两种上下文环境下发挥的句法语义功能完全不同。\cite{guo-EtAl:2014:Coling}
给予不同环境中的两个词
以相同的表示做法
无疑是值得商榷的。

动态上下文相关词向量\cite{NIPS2017_7209,peters-EtAl:2018:N18-1,P18-1031,gpt1,DBLP:journals/corr/abs-1810-04805}作为静态词向量的一种改进，取消了
``一个词有唯一的词向量''的假设，
在不同的上下文环境中，赋予相同的词不同的词向量，进而建模了
一个词上下文环境相关的句法语义差异。
这种词的向量化的技术在2017年末被提出，
并在2018年引起广泛关注，
迅速成为自然语言处理领域的热点。
%因成功地建模了一个词在不同上下文中的语义，
%上下文相关词向量技术迅速成为2018年自然语言处理领域的热点。
在其兴起的过程中，
两项代表性工作扮演了重要的角色。
首先，Peters等人在2018年的文献\inlinecite{peters-EtAl:2018:N18-1}中
提出\textit{\elmochinesetranslation}（Embeddings from Language Modeling，简称ELMo）。
文献\inlinecite{peters-EtAl:2018:N18-1}
将一个词及其所在的句子输入
使用\text{长短时记忆循环神经网络}（long short-term memory network，
简称LSTM，文献\inlinecite{Hochreiter:1997:LSM:1246443.1246450}）
建模的双向语言模型，
并将得到的隐层作为对应上下文相关词向量。
在文献\inlinecite{peters-EtAl:2018:N18-1}发表时，
这种词向量帮助
包括\text{问答}、\text{文本蕴含}、\text{语义角色标注}、
\text{共指消解}、\text{命名实体识别}以及\text{情感分析}在内的六项自然语言处理任务中取得了当时最好的结果。
在文献\inlinecite{peters-EtAl:2018:N18-1}发表后不久，
Devlin等人在2018年的文献\inlinecite{DBLP:journals/corr/abs-1810-04805}中
提出使用\text{自注意力网络}（self-attention network，简称Transformer，文献\inlinecite{NIPS2017_7181}）
建模上下文，
同时用词级别与句子级别的\text{完形填空}作为学习目标
的\textit{基于自注意力网络的上下文相关词向量}
（Bidirectional Encoder Representations from Transformers，简称BERT）。
在11项任务中，
使用BERT的模型超越了使用ELMo的模型，取得了当前最好的结果。
种种结果均表明，
上下文相关词向量是近期提出的
一种提高自然语言处理性能的
有效手段。

包括分词\cite{Xue:2003:CWS:1119250.1119278,zhang-clark:2007:ACLMain,zheng-chen-xu:2013:EMNLP}、
词性标注\cite{Toutanova:2003:FPT:1073445.1073478,DBLP:journals/corr/HuangXY15,ma-hovy:2016:P16-1}、
命名实体识别\cite{Ando:2005:FLP:1046920.1194905,lample-EtAl:2016:N16-1}、
句法分析\cite{mcdonald2006online,nivre2008algorithms,zhang-clark:2008:ACLMain,chen-manning:2014:EMNLP2014,DBLP:journals/corr/DozatM16}
在内句子级
语言分析问题是自然语言处理的基石。
语言分析的性能很大程度上影响了
包括
语义角色标注\cite{W04-3212,li-etal-2010-joint}、
机器翻译\cite{dyer-2009-using,aharoni-goldberg-2017-towards}、
信息抽取等
后续任务的性能。
现阶段，基于统计学习的语言分析算法已经成为主流，
而基于神经网络的方法更是应用于自然语言处理中的一种重要的统计学习手段。
得益于词的成功表示，使用``静态''词向量的作为输入的
语言分析模型已经在相应任务上取得了很大的成功。\cite{zheng-chen-xu:2013:EMNLP,chen-manning:2014:EMNLP2014,DBLP:journals/corr/HuangXY15,ma-hovy:2016:P16-1,DBLP:journals/corr/DozatM16}
然而，\textbf{在语言分析中使用``动态''的上下文相关词向量尚未获得充分研究。
上下文相关词向量与语言分析技术的结合
中有诸多有趣并值得探索的问题。}

\subsection{课题意义}[Significance]

自然语言是人机交互的一种重要手段，
同时也是机器算法
感知分析人类社会的一种重要途径。
处理自然语言是实现机器智能的重要一步。
而语言分析作为其他自然语言任务的第一步，
其准确率、效率对于处理自然语言有很大的影响。
同时，
大部分语言分析问题衍生于语言学问题，
研究语言分析问题可以帮助我们更好地研究人类语言。

时至今日，
统计学习已经成为语言分析乃至整个自然语言处理的主流方法。
影响统计学习模型效果的一个重要的因素是
对模型输入进行合理的表示。
可以说，
基于统计学习的语言分析模型的发展是与表示方法的发展紧密相关的。
从早期基于统计量的表示方法\cite{Brown:1992:CNG:176313.176316,Brown:1993:MSM:972470.972474,Vogel:1996:HWA:993268.993313,eisner-1996-coling}
到基于大规模离散特征的表示方法\cite{collins:2002:EMNLP02,mcdonald2006online,daume05search,zhang-clark:cl:2011}
再到现今流行的基于
词向量的表示方法\cite{NIPS2013_5021,DBLP:journals/corr/HuangXY15,kim-rush:2016:EMNLP2016}
语言分析模型也从适应离散特征的线性模型\cite{NIPS2003_2397,Crammer:2006:OPA:1248547.1248566}
发展到
适应向量化表示的神经网路模型\cite{hubel:monkey,Hochreiter:1997:LSM:1246443.1246450,Collobert:2011:NLP:1953048.2078186}。
而作为现今广泛应用的词的表示方法，
词向量的质量很大程度上影响了基于神经网络的语言分析模型
的性能。\cite{faruqui-EtAl:2016:RepEval,7478417,schnabel-EtAl:2015:EMNLP}
成功建模句法语义信息的词向量能帮助
语言分析模型取得更好的准确率。
作为一种新兴的词表示技术，
上下文相关词向量
已经在一系列自然语言处理任务中表现出
巨大的潜力。
其对于语言分析问题
的作用非常值得研究。

以语言分析问题为场景
研究上下文相关词向量的意义是多方面的。
其中最主要的意义是
研究\textbf{通过上下文相关词向量
提高语言分析模型的准确率}。
为了提高准确率，
即需要研究\textit{如何将上下文相关词向量应用在
语言分析问题中}，
也需要研究\textit{为语言分析问题
设计更有效的上下文相关词向量}。
其次，由于大部分语言分析问题衍生于语言学问题，
在语言分析问题中研究上下文相关词向量
可以\textbf{分析并理解上下文相关词向量的性质}，
进而指导其在其他任务中的应用。
最后，
将基于上下文相关词向量的语言分析模型
部署到实际应用时
除了要关心性能，还要关心运行效率、资源开销等方面的问题。
从这些方面
研究上下文相关词向量可以\textbf{使上下文相关
词向量在实际场景中得到更广泛的应用}。

从上面的研究意义出发，
本文拟围绕上下文相关词向量与语言分析问题
开展一系列研究。
1）为了通过上下文相关词向量提高语言分析的性能，
本文将上下文相关词向量与现有系统融合以提高这些系统的性能，
并探索基于上下文相关词向量构造自然语言结构（序列、片段、树等）
表示的可能性。
2）为了分析并理解上下文相关词向量的性质，
本文在大规模语言分析问题上
应用并分析上下文相关词向量的影响。
3）为了使上下文相关词向量在实际场景中得到更广泛的应用，
本文提出一种适应语言分析
的高效上下文相关词向量算法。
同时提出一种模型压缩的方法以实现又快又好的
语言分析。

\section{研究现状与分析}[Related Work]

以语言分析问题为场景
研究上下文相关词向量包含两方面研究主体，
即：上下文相关词向量和语言分析技术。
在接下来的章节，
本文将回顾与分析这两个主体的研究现状。
具体来讲，本文将首先
回顾上下文相关词向量的前身``静态/上下文无关词向量''，
将其与上下文相关词向量建立联系（\S\ref{sec:intro:review:static-emb}）；
然后
分析现阶段主流的上下文相关词向量
的算法与性能（\S\ref{sec:intro:review:cont-emb}）；
最后
结合词向量分析现阶段语言分析技术
的发展情况，
并讨论其与上下文相关词向量潜在的结合方向（\S\ref{sec:intro:review:lang-anal}）。

\subsection{静态词向量}[Static Word Embeddings]\label{sec:intro:review:static-emb}
如前文所述，词的向量化表示是基于统计学习的自然语言处理的基础。
广义上讲，所有将词转化为向量的算法都可以归类为词的向量化算法。
这些算法中包括传统``特征工程''的方法，即根据一系列专家定义的
启发式规则将词（以及其形态学、上下文等）特征转换为高维、稀疏、
并且通常是离散的向量。
随着，基于神经网络的自然语言处理的发展，
越来越多的自然语言处理问题通过神经网络获得了更好的性能。
所以，本文将注意力集中在基于神经网络的方法上。
因而，本文关注的词的向量化的算法特指
将词转化为低维、稠密、连续的向量的算法。

在讨论将词转化为低维向量表示的算法前，
一个值得讨论的问题是：\textit{为什么词能够通过低维、稠密、连续的向量
进行表示？}
从一定程度上讲，这一问题可以
通过特征工程中\textit{降维}思想进行回答。
降维可以将高维、稀疏、离散的具有可解释性的特征向量
近似表示为低维、稠密、连续的向量。
从实际应用的角度，这种降维后的向量往往具有不输于离散表示的性能，
甚至在一些任务中取得了更好的性能。\cite{lei-EtAl:2014:P14-1,lei-EtAl:2015:NAACL-HLT}
基于降维的词表示算法
经验性地证明了词具有低维、稠密、连续的向量表示。
基于上述论述，本文假设词具有低维、稠密、连续的向量表示，即词向量。

为了使词向量能够在实际应用中发挥作用，
普遍接受的观点是词向量应该携带词的句法语义信息。\cite{NIPS2013_5021,faruqui-EtAl:2016:RepEval,7478417}
这种句法语义信息通常是通过建模词义的分布式假设来实现的。
接下来，本文将从\textit{建模词义分布式假设}的角度出发，
分析一系列\textbf{静态/上下文无关}的词向量算法。\footnote{这里的
``上下文无关''是从\textit{一个词向量化的过程是否使用其上下文}的角度出发的。
由于词向量的学习过程必定依赖其上下文，
根据词向量的学习过程区分是否上下文相关并无太大意义。}
%\yjcomment{逻辑上从携带句法语义信息到词义分布式假设之间不是特别直接，不过也想不到更好的过渡方式了。}

\subsubsection{使用``共现矩阵''建模分布式假设的方法}[Methods that Inspired by Concurrence Matrix]\label{sec:intro:review:static-emb:matrix}
\begin{figure}[t]
	\wuhao
	\[
	\begin{bmatrix}
	 & \text{barn} & \text{fell} & \text{horse} & \text{past} & \text{raced} & \text{the} \\
	\text{barn} & 1 & 1 & 0  & 0 & 0 & 1 \\
	\text{fell} & 1 & 1 & 0  & 0 & 0 & 0 \\
	\text{horse} & 0 & 0 & 1  & 0 & 1 & 1 \\
	\text{past} & 0 & 0 & 0  & 1 & 1 & 1 \\
	\text{raced} & 0 & 0 & 1 & 1 & 1 & 0 \\
	\text{the} & 1 & 1 & 0  & 1 & 0 & 2 \\
	\end{bmatrix}
	\]
	\bicaption{}{使用词的上下文窗口构造共现矩阵的一个例子\cite{lund1996producing}。
	这个例子为``the horse raced past the barn fell''建立了窗口大小为2的共现矩阵。}{Fig.~$\!$}{An example of the concurrence matrix which models the sentence ``the horse raced past the barn fell''
	with a window of 2 words.\label{fig:intro:review:example}}
\end{figure}
词义分布式假设强调``上下文相似的词有相似的词义''。
词义分布式假设包含两部分主体：
一个是\textit{上下文}，
另一个是被描述的词（下文称``\textit{目标词}''）。
建模词义分布式假设本质上是：
1）选择一种方式描述上下文；
2）选择一种模型描述目标词与其上下文之间的关系。

最简单的描述目标词与上下文关系的方法是
关注目标词与哪些词或文档共现。
潜在语义分析（Latent semantic analysis，简称LSA）是这类方法的代表。
最早，LSA提出的目标是建模信息检索中词义相似的词。
LSA中的一个目标词首先被表示为一个向量。
向量的维度与文档的个数相等。
一个词的向量的某一维表示这个词在对应文档中的某种计量。\footnote{这种计量可以是频次，tf-idf，点互信息等。}
然后，所有词被堆叠成为一个矩阵，即\textit{共现矩阵}。
最后经过矩阵分解技术，
原始的词向量被映射到低维的表达词的潜在语义的空间。
奇异值分解（Singular Value Decomposition，简称SVD）
是这类方法中最常用的矩阵分解技术。
%其假设高维到低维之间存在线性变换。
从建模分布式假设的角度来看，
LSA使用目标词所在的文档描述目标词的上下文。

除了使用文档间接地描述上下文，
目标词周围的词也可以描述上下文，从而构造共现矩阵。
Lund与Curt在1996年的文献\inlinecite{lund1996producing}中
提出Hyperspace Analogue to Language（简称HAL）算法，
首次使用目标词与其周围的词的共现次数建模目标词的词义
（如图\ref{fig:intro:review:example}所示）。
Rohde等人在2006年的文献\inlinecite{Rohde06animproved}中
提出COALS算法并对HAL进行了一系列改进。
这些改进包括：将共现矩阵中的计数转化为皮尔逊相关系数（Pearson's correlation）
并去掉负相关的共现，同时使用SVD分解获得词的低维的向量化表示。
文献\inlinecite{Rohde06animproved}可以认为是使用
目标词与周围词的关系计算低维、连续、稠密词向量的先驱工作。
在这之后，多种基于共现矩阵建模词义分布式假设的词向量算法
被提出。其中包括Hellinger PCA算法\inlinecite{lebret-collobert:2014:EACL}，
GloVe算法\inlinecite{pennington-socher-manning:2014:EMNLP2014}等。
值得一提的是，
Pennington等人在2014年的文献\inlinecite{pennington-socher-manning:2014:EMNLP2014}中
提出的GloVe算法也使用共现矩阵建模了词义分布式假设。
但不同于采用矩阵分解降维的工作，
GloVe算法
首先假设目标词存在低维向量化表示，
然后将向量化的表示输入一个函数并用回归的方式对共现矩阵
中的值进行拟合以达到降维（或向量化）的效果。

Word2vec\cite{NIPS2013_5021}是另一个流行的向量化算法。
Word2vec算法可以概括为将目标词的向量化的表示输入一个神经网络
预测其在生语料中的上下文。
可以说，Word2vec是使用``预测目标词或上下文''的思路建模分布式假设。
但Levy等人在2014年的文献\inlinecite{NIPS2014_5477}中证明
使用负采样（negative sampling）优化的Word2vec
等价于对于目标词及其窗口内上下文的点互信息矩阵进行SVD分解。
这一发现将基于``共现矩阵''与基于``预测目标词或上下文''的方法进行了统一。
后文会从``预测目标词或上下文''的角度对Word2vec进行讨论。

\subsubsection{使用``预测目标词''建模分布式假设的方法}\label{sec:intro:review:static-emb:predict}
除了通过目标词$w_i$与其上下文$\{w_j\}$的共现描述两者关系，
\textit{通过上下文预测目标词}或者
\textit{通过目标词预测上下文}
也是一种描述两者关系
的常用方法。
这种方法
的第一步是定义一个词到其向量的映射关系$\phi$。
将一个词$w_i$及其上下文的$w_j$输入到$\genericwordvecfunc$中
获得他们的向量表示$\symbolvec_i$与$\{\symbolvec_j\}$，
并把这些向量输入到一个建模上下文的函数$\genericcontextfunc$；
从而获得其上下文表示$\ctx_i=\genericcontextfunc (\{\symbolvec_j\})$，
最后将$\ctx_i$输入到一个预测函数$\mathcal{F}_{p} (\ctx_i, \symbolvec_i)$
达到描述两者关系的目标。
在这类方法中，
$\phi$是词向量算法的最终产出。

\begin{figure}[t]
	\centering
	\includegraphics[width=0.65\columnwidth, trim={0 0 12.2cm 11cm},clip]{intro/bengio03}
	\bicaption{}{Bengio等人在2003年的文献\inlinecite{Bengio:2003:NPL:944919.944966}中提出的语言模型}{Fig.~$\!$}{The language model proposed by Bengio et al. (2003) \inlinecite{Bengio:2003:NPL:944919.944966}\label{fig:intro:bengio03}}
\end{figure}

\paragraph{基于语言模型的方法}
前人工作探索了不同的\textit{上下文函数}$\genericcontextfunc$与不同\textit{预测函数}$\genericpredictfunc$。
一种典型的组合是使用前文表示上下文，根据前文预测下一个词，
即\textit{语言模型}。
Bengio等人在2003年的文献\inlinecite{NIPS2000_1839}中提出
一种利用有限窗口上文预测下一个词的语言模型（如图\ref{fig:intro:bengio03}所示），
成为这一类方法的先驱。
具体来讲，给定一个窗口大小为$W$的上文$w_{i-W-1}, \dots, w_{i-1}$以及目标词$w_{i}$，
文献\inlinecite{NIPS2000_1839}使用向量拼接的方式建模$\genericcontextfunc$，
并使用多层神经网络建模用上文预测下一个词的概率
$\genericpredictfunc = p(w_{i} \mid w_{i-W-1}, \dots, w_{i-1})$。
其模型可以形式化地描述为，
\begin{align*}
\ctx_i &= \oplus_{j=i-W-1}^{i-1} \mathbf{v}_j \\
\mathbf{s} &= \mlpfnpp{\ctx_i} + \mlpfn{\tanh(\mlpfn{\ctx_i})}\\
p(w_{i} \mid w_{i-W-1}, \dots, w_{i-1}) &= \softmaxfn{w}{\mathbf{s}_{w_{i}}}\text{。}
\end{align*}
其中，$\oplus$代表向量的顺序拼接。
对Bengio的模型中的预测函数$\genericpredictfunc$与
上下文函数$\genericcontextfunc$进行了一系列改进。
这些改进包括：
Mnih与Hinton在2007年的文献\inlinecite{Mnih:2007:TNG:1273496.1273577}
中提出的在建模上下文$\genericcontextfunc$时使用加权求和
取代拼接的Log-bilinear Language Model（简称LBL），
以及
Mnih与Hinton在2007年的文献\inlinecite{NIPS2008_3583}中
提出的
使用基于词表聚类的启发式负采样
提高LBL中预测函数$\genericpredictfunc$
的学习速度的
Hierarchical Log-Bilinear Language Model。

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\columnwidth, trim={0 0 12.2cm 12.5cm},clip]{intro/rnnlm}
	\bicaption{}{基于循环神经网络的语言模型}{Fig.~$\!$}{The recurrent neural network language model\label{fig:intro:rnnlm}}
\end{figure}
除了使用简单拼接或者加权平均的方法建模上下文$\genericcontextfunc$，
循环神经网络（recurrent neural network，简称RNN）
具备建模任意长度历史的能力，
因而也可以作为
上下文的一种建模。
Mikolov等人在2010年的文献\inlinecite{DBLP:conf/interspeech/MikolovKBCK10}
中提出RNN语言模型建模词的上下文（如图\ref{fig:intro:rnnlm}所示）。
与文献\inlinecite{NIPS2000_1839}的思路一样，RNN语言模型
也尝试建模一定上文条件下一个词出现的概率。
具体来讲，其形式如下：
\begin{align*}
\symbolvec_i &= \text{RNN}(\mathbf{v}_1, \dots, \mathbf{v}_{i-1}) \\
\mathbf{s} &= \mlpfn{\symbolvec_i} \\
p(w_{i} \mid w_1, \dots, w_{i-1}) &= \softmaxfn{w}{\mathbf{s}_{w_{i}}}\text{。}
\end{align*}

可以说，
语言模型是最早的成功建模上下文以及上下文与目标词关系的方法。
通过分析可见，
语言模型的思想在
在上下文相关词向量中也得到了应用。

\paragraph{基于完形填空的方法}
Collobert等人在2011年的文献\inlinecite{Collobert:2011:NLP:1953048.2078186}
中提出了另一种基于``完形填空''的描述上下文以及上下文与目标词关系的方法，
即包含目标词的上下文的``真实程度''大于将目标词替换为随机词的上下文的真实程度。
具体来讲，
对于窗口为$2W$的上下文，
文献\inlinecite{Collobert:2011:NLP:1953048.2078186}
使用卷积神经网（convolutional neural network，简称CNN，文献\inlinecite{hubel:monkey}）
建模上下文$\mathcal{F}_{c}$，
并将上下文向量输入一个前馈神经网络中获得一个标量$s$作为这一上下文``真实程度''的一种度量。
具体来讲，
一段上下文$\symbolvec_{i-W}, \dots, \symbolvec_{i}, \dots, \symbolvec_{i+W}$的``真实程度''
可以描述为：
\begin{align}\label{eq:intro:collobert}
\ctx_i &= \text{CNN}(\symbolvec_{i-W}, \dots, \symbolvec_{i}, \dots, \symbolvec_{i+W}) \nonumber\\
s_{w_i} &= \mlpfnpp{\tanh(\mlpfn{\ctx_i})}\text{。}
\end{align}
为了训练模型参数，%文献\inlinecite{Collobert:2011:NLP:1953048.2078186}提出
%一种基于完形填空的训练思想，即真实的上下文的得分应高于将目标词随机替换后获得的上下文。
文献\inlinecite{Collobert:2011:NLP:1953048.2078186}
使用最大化分类边界学习目标（margin loss）：
\begin{align}
\genericparams = \argmin_{\genericparams} \max \{0, 1 - s_{w_i} + s_{w'}\}
\end{align}
建模了这种完形填空的训练思想。
其中$s_{w'}$为将在上下文$w_{i-W}, \dots, w_i, \dots w_{i+W}$中的$w_i$
替换为$w'$后重新计算公式\ref{eq:intro:collobert}获得的分数。

文献\inlinecite{Collobert:2011:NLP:1953048.2078186}提出的完形填空的思想有诸多独到之处。
相对语言模型来讲，
其最为重要的不同是在描述上下文时不只考虑上文，更可以考虑下文。
可以说，这种思想与Devlin等人在文献\inlinecite{DBLP:journals/corr/abs-1810-04805}中
提出的BERT上下文相关词向量有紧密的联系。

\paragraph{基于单个上下文词与目标词关系的方法}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth, trim={0 0 12.5cm 9cm},clip]{intro/word2vec}
	\bicaption{}{文献\inlinecite{DBLP:journals/corr/abs-1301-3781}提出的两种模型：CBOW（左）与Skip-gram（右）}
	{Fig.~$\!$}{The CBOW model (left) and Skip-gram model (right) from Mikolov et al. (2013) \inlinecite{DBLP:journals/corr/abs-1301-3781}\label{fig:intro:word2vec}}
\end{figure}

如前文所述，建模词义分布式假设的核心思想由两部分组成：
1）选择一种方式描述上下文；
2）选择一种模型刻画目标词与其上下文之间的关系。
使用语言模型以及使用完形填空的方法
都对一定宽度内上下文整体进行了表示。
能否将上下文的整体表示化简为单个独立的词
并预测单个上下文词与目标词之间的关系呢？
Mikolov等人在2013年文献\inlinecite{NIPS2013_5021}中
基于这种思想
提出了Word2vec模型。

相较语言模型，Word2vec模型采用某个上下文词的词向量$\wordvec'_j$作为
$\genericcontextfunc$，
并使用
目标词预测这个上下文或这个上下文预测目标词的概率作为$\genericpredictfunc$。
文献\inlinecite{NIPS2013_5021}提出两种
模型建模，分别是CBOW与Skip-gram模型（参考图\ref{fig:intro:word2vec}）。
CBOW建模的是根据上下文词的评价预测目标词的概率，即$p(w_{i} \mid w_{i-W}, \dots, w_{i-1}, w_{i+1}, \dots, w_{i+W})$；
Skip-gram建模的是根据目标词预测某个上下文的概率，即$p(w_j \mid w_i)$，其中$i-W\le j\le i+W, j \ne i$。
具体来讲，CBOW模型可以描述为
\begin{align*}
\ctx_i &= \frac{1}{2n}\sum_{i-W\le j\le i+W, j \ne i} \wordvec'_j \\
p(w_{i} \mid w_{i-W}, \dots, w_{i-1}, w_{i+1}, \dots, w_{i+W}) & = \softmaxfn{w}{\left(\mathbf{v}_i\right)^{T} \cdot \ctx_i}\text{。}
\end{align*}
而Skip-gram可以描述为
\begin{align}
p(w_j \mid w_i) = \softmaxfn{w}{\left(\mathbf{v}_i\right)^T \cdot \mathbf{v}'_j} \text{。}
\end{align}
需要说明的是，文献\inlinecite{NIPS2013_5021}
采用两套不同的词向量分别建模了上下文词向量与预测词向量。

Word2vec算法作为基于神经网络的自然语言处理模型的基础
对于自然语言处理的影响是深远的。
但Word2vec对于词、上下文以及外部知识的过度简化也激发了
一系列工作。
1）由于Word2vec在建模词时没有考虑词的形态学特征，
一系列工作尝试将词的形态学信息融入Word2vec中。
FastText\cite{Q17-1010}
是这类方法中最典型的代表。
相比Word2vec采用的给每个词一个词向量的做法，
FastText给词内字级别的ngram一个向量，
并用ngram向量相加的方式获得词向量。
相较Word2vec，
FastText可以给任意词一个向量，因而具有处理未登录词的能力。
2）由于Word2vec没有考虑目标词与上下文词的相对位置，
Wang等人在文献\inlinecite{ling-EtAl:2015:NAACL-HLT}中
提出使用不同的$\phi$表示不同相对位置的词，从而改进了原始算法。
3）由于Word2vec在建模词义时没有考虑人工构建的词义词典，
一系列工作也尝试将
外部知识，比如WordNet\cite{rothe-schutze:2015:ACL-IJCNLP}，
双语词典\cite{DBLP:journals/corr/SmithTHH17}，
平行语料\cite{AAAI1612236}，
远程监督信息\cite{tang-EtAl:2014:P14-1}等，
引入Word2vec的模型学习中。

\paragraph{静态词向量的评价}
至此，本章已经对当前具有代表性的``静态''词的向量化算法进行了回顾。
一个自然产生的问题是比较这些向量化的算法的优劣。
Lai等人在2016年的文献\inlinecite{7478417}中对这些``静态''词向量化算法从多个方面进行了比较。
文献\inlinecite{7478417}比较的内容包括不同的因素对于产生的词向量的质量的影响。
文献\inlinecite{7478417}采用语义相似度、语义类比，用作特征时的模型性能，用作初始化时的模型性能
三类指标评价了不同词向量的性能。
这一系列工作尝试回答：1）训练语料规模对于模型性能的影响；
2）相同的参数设置下，哪种模型性能更好。
对于第一个问题，文献\inlinecite{7478417}的结论是
是训练数据规模越大，得到的词向量性能越好。
对于第二个问题，他们的研究没有给出明确的答案。
不同的词向量算法在不同的实验指标下
表现并不一致。
Yin与Schutze也在2016年的文献\inlinecite{yin-schutze:2016:P16-1}
中验证了这一观察。

\begin{table}[t]
\centering
\bicaption{}{前人工作中的不同词向量的性能}{Table~$\!$}{A comparison on the static word embedding algorithms\label{tbl:intro:emb-on-task}}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{r | p{4.3cm}p{2.8cm} | cc | c}
	\toprule[1.5pt]
	 & \multirow{2}{*}{上下文表示} &  \multirow{2}{2.5cm}{目标词与上下文的关系} & \multicolumn{2}{c|}{文献\inlinecite{7478417}} & 文献\inlinecite{yin-schutze:2016:P16-1} \\
	 &  &  & NER & POS & POS \\
	\midrule[1pt]
	Random & & & 84.39 & 95.41 & - \\
	\midrule[0.5pt]
	GloVe & 加权的共现比例 & 预测共现比例 & 88.19 & 96.42 & 96.65 \\
	Skip-gram & 上下文某个词的词向量 & 预测目标词& \textbf{88.90} & 96.57 & - \\
	CBOW & 上下文各词词向量的平均值& 预测目标词& 88.47 & 96.63 & 96.40 \\
	LBL & 上下文各词的语义组合& 预测目标词& 88.69 & \textbf{96.77} & - \\
	HLBL& 上下文各词的语义组合& 预测目标词& - & - & 96.53 \\
	NNLM & 上下文各词的语义组合& 预测目标词& 88.36 & 96.73 & - \\
	C\&W & 上下文各词与目标词的语义组合& 目标词与上下文的联合打分& 88.15 & 96.66 & 96.58 \\
	\bottomrule[1.5pt]
\end{tabular}
\end{table}
值得一提的是，词向量化的评价是一个争议很大的问题。
词义相似度是很多词向量算法使用的评价指标。
然而，Faruqui等人在文献\inlinecite{faruqui-EtAl:2016:RepEval}中
指出词义相似度有诸多问题，比如：词义相似度的评价具有主观性，
与特定任务关联较强，与下游任务的性能相关性较弱，
不能评价一词多义的现象等。

本文关注的主要问题是词向量与下游语言分析任务的关系，
一般来讲，
词向量在神经网络中的应用一般有两种模式：
1）\textit{做为特征}，
以及2）\textit{做为神经网络的初始化}。
第一种模式将词向量用作输入，
并在模型学习中保持不变。
第二种模式用词向量初始化参数。
第一种模式使
模型可以给未出现在训练数据中的词（指的是特定任务的训练数据）
一个合理的词向量。
这种方式往往给模型带来更好的泛化性，
但由于词向量的泛化能力问题，也给模型学习带来了困难。
第二种模式往往使模型更容易学习，但也牺牲了一定的泛化性。
两种方式孰优孰劣无法从理论上给出解释，
大部分工作是依靠实际任务的性能进行评价的。

关于不同的词向量在实际任务中的表现一直众说纷纭。
文献\inlinecite{7478417}与文献\inlinecite{yin-schutze:2016:P16-1}
都讨论了这一问题。
表\ref{tbl:intro:emb-on-task}总结了他们的比较结果。
两篇文献都在相同的数据规模的设置下重新训练了词向量。
其中，文献\inlinecite{7478417}在CoNLL03命名实体识别数据集\cite{TjongKimSang-DeMeulder:2003:CONLL}上将
词向量用作特征，在Penn Treebank （简称PTB）词性标注数据集\cite{Marcus93buildinga}上将词向量用作初始化。
文献\inlinecite{yin-schutze:2016:P16-1}在PTB词性标注数据集
上将词向量用作初始化。
从表\ref{tbl:intro:emb-on-task}可以看出，不同的词向量的性能存在一定差异，但这种差异并不显著，
同时某种词向量并不稳定地好于其他词向量。
究其原因，本文认为主要的问题在于
这类方法采用静态的词向量表示方式，即一个词只有唯一的向量化表示。
这种表示方式使得词向量无法显示地建模一个词在不同上下文环境下
句法语义功能的不同。

\subsection{上下文相关词向量}[Contextualized Word Embeddings]\label{sec:intro:review:cont-emb}
一个词在不同的上下文环境下发挥的句法语义功能可能存在很大差异。
举例来讲，``制服''在``他\textit{制服}了窃贼''与``身穿该厂\textit{制服}的工人''
两种上下文环境下分别用作动词（代表``用强力使之驯服''）以及名词（代表``有规定式样的服装''）。
给两种上下文环境下的``制服''相同的向量表示的做法无疑是值得商榷的。
为了建模这种与上下文相关的句法语义功能，
更合理的向量化方法是根据一个词及其上下文环境
动态地决定它的向量表示。
接下来，本章将按照上下文相关词向量的发展过程对前人研究进行回顾。

\paragraph{context2vec}
Melamud等人在2016年的文献\inlinecite{melamud-goldberger-dagan:2016:CoNLL}
中首次从词向量的角度讨论这一问题
并提出context2vec算法。
context2vec在描述预测目标词与上下文关系时
可以类比CBOW模型。
但不同的是，context2vec采用LSTM描述
上下文$\mathcal{F}_c$。
具体来讲，context2vec将
从左向右以及从右向左的词
分别输入两个LSTM中，
并将隐层输出作为上下文的表示。
context2vec可以形式化地定义为：
\begin{align*}
\ctxp_i &= \elmoleftctxfn(\wordvec_{1}, \dots, \wordvec_{i-1}) \oplus \elmorightctxfn(\wordvec_{i+1}, \dots, \wordvec_{n}) \\
\ctx_i & = \mlpfnpp{\text{ReLU}\left(\mlpfn{\ctxp_i}\right)} \\
p(w_i \mid w_1, \dots, w_{i-1}, w_{i+1}, \dots, w_{n}) & = \softmaxfn{w}{(\wordvec_i)^T \cdot \ctx_i}\text{。}
\end{align*}
除了选择不同的方式描述上下文，
相对Word2vec一类的静态词向量，
context2vec最重要的不同在于\textbf{采用$\ctx_i$
作为$w_i$的词向量}。
这种做法使得同一个词在不同的上下文条件下拥有不同的向量表示。
可以说，context2vec是上下文相关词向量的最早尝试。

\paragraph{CoVe}
McCann等人在2017年的文献\inlinecite{NIPS2017_7209}中
再次强调了依照上下文对词进行向量化的重要性。
在文献\inlinecite{NIPS2017_7209}中，
McCann等人
使用神经网络的机器翻译\cite{NIPS2014_5346,luong-pham-manning:2015:EMNLP,DBLP:journals/corr/BahdanauCB14}的
解码器的输出作为对应词的上下文相关词向量，
并将其算法命名为上下文相关词向量（Contextualized word vectors，简称CoVe）。
虽然并未明确定义目标词与上下文的，
可以认为CoVe使用双向LSTM描述上下文$\mathcal{F}_c$，
用机器翻译中目标语的词作为目标词，
从而使用翻译的关系描述了源语言上下文与目标语词之间的关系。
McCann等人将CoVe词向量作为特征输入，
并在情感分析\cite{socher-EtAl:2013:ACL2013}、
问题分类\cite{DBLP:conf/trec/Voorhees99}、
文本蕴含\cite{bowman-EtAl:2015:EMNLP}
以及阅读理解\cite{rajpurkar-EtAl:2016:EMNLP2016}
中取得了性能的提升。

\paragraph{ELMo}
从建模词义分布式假设的角度讲，
CoVe并未对目标词以及上下的关系进行明确定义。
2018年，Peters等人在文献\inlinecite{peters-EtAl:2018:N18-1}中
明确将目标词定义为语言模型中待预测的下一个词，
并提出\elmochinesetranslation --- ELMo。
具体来讲，一个句子$\nwords$的前向语言模型可以形式化地定义为：
\begin{align}
p(w_i \mid w_1, \dots, w_{i-1}) = \softmaxfn{w}{W_{w_i} \leftctx_{i-1} + b_{w_i}}\text{。}
\end{align}
其中$\leftctx_i$是前向上下文表示模型的最终输出。
后向语言模型采用相同的方式定义。

为了建模语言模型并计算$\leftctx_i$，
文献\inlinecite{peters-EtAl:2018:N18-1}
首先使用基于字的CNN计算一个词的上下文无关的表示，即$\wordvec_{i} = \text{CNN}(w_{i})$；
然后使用带有短路机制（skip-connections，文献\inlinecite{DBLP:journals/corr/HeZRS15}）
的多层LSTM表示前向/后向上下文。
以前向语言模型为例，多层模型可以递归地定义为
\begin{align}
\leftctx_{i}^{(k)} = \elmoleftctxfn^{(k)}\left( \leftctx_{1}^{(k-1)}, \dots, \leftctx_{i}^{(k-1)}\right)\text{。}
\end{align}
其中，$\leftctx_{i}^{(0)} = \wordvec_{i}$ and $\rightctx_{i}^{(0)} = \wordvec_{i}$；
最后，上下文相关词向量可以定义为$L$层隐层状态的加权平均，即
\begin{align}\label{eq:intro:elmo:pool}
	\text{ELMo}_i = \gamma \sum_{k=0}^L s_k \cdot \left(\leftctx_i^{(k)} \oplus \rightctx_i^{(k)}\right)\text{。}
\end{align}

文献\inlinecite{peters-EtAl:2018:N18-1}
提出通过最大化下一个词的对数似然的方式在大规模生文本中学习语言模型。
最终学习目标是两个方向上的对数似然的加和，
形式化地定义为：
\begin{align}\label{eq:intro:elmo:objective}
\genericparams &= \argmax_{\genericparams} \sum_{i=1}^{n} \left( \log p(w_i \mid w_1, \dots, w_{i-1}) +  \log p(w_i \mid w_{i+1}, \dots, w_n) \right) \nonumber\\
& =\argmax_{\genericparams} \sum_{i=1}^{n} \log \softmaxfn{w}{\mlpfn{\leftctx_i}} + \log \softmaxfn{w}{\mlpfn{\rightctx_i}} \textit{。}
\end{align}

文献\inlinecite{peters-EtAl:2018:N18-1}将上下文相关词向量用作特征，
在包括阅读理解\cite{rajpurkar-EtAl:2016:EMNLP2016}、
文本蕴含\cite{bowman-EtAl:2015:EMNLP}、
语义角色标注\cite{W13-3516}、
共指消解\cite{W12-4501}、
命名实体识别\cite{TjongKimSang-DeMeulder:2003:CONLL}、
以及情感分析\cite{socher-EtAl:2013:ACL2013}
在内的六项数据集上取得了当时最优结果。

除了性能的提升，文献\inlinecite{peters-EtAl:2018:N18-1}也指出
对于多层LSTM，使用语言模型为目标学习出的网络具有
从句法到语义的逐步抽象能力。
具体来讲，LSTM的底层有更好的句法抽象能力，高层具有
更好的语义抽象能力。\footnote{实践中，底层在词性标注等典型句法任务中表现更好，高层在共指消解等典型语义任务中表现更好。}
同年，Peters等人在文献\inlinecite{peters-EtAl:2018:EMNLP}中
对于ELMo进行了更加深入的分析。
根据文献\inlinecite{peters-EtAl:2018:EMNLP}，
ELMo的成功主要来自从大规模生语料中使用语言模型的
学习目标学习参数。
表示上下文的网络结构对于最终模型性能的影响并不大。\footnote{文献\inlinecite{peters-EtAl:2018:EMNLP}尝试了Gated CNN \cite{pmlr-v70-dauphin17a}，
	Transformer \cite{NIPS2017_7181}两种网络结构。}
同时，不同的网络都表现出底层的句法抽象能力更好，
高层的语义抽象能力更好的特性。

\paragraph{ULMFit}
在ELMo同时，Howard与Ruder在2018年的文献\inlinecite{P18-1031}中
从迁移学习的角度研究了从语言模型迁移到文本分类任务的最佳实践方法。
文献\inlinecite{P18-1031}强调
前人有关语言模型未能给文本分类带来显著性能提升的
主要原因是并未合理地对语言模型进行\finetunechinesetranslation（fine-tune parameters）。
文献\inlinecite{P18-1031}
从另一个角度佐证了上下文相关词向量的有效性。

\paragraph{GPT}
可以看出，从描述上下文到描述目标词与上下文的关系，
ELMo都与第\ref{sec:intro:review:static-emb:predict}节中基于RNN的语言模型有密切关系。
不同于前人工作将RNN语言模型中的词向量表视作
向量化的最终产物，
ELMo将整个RNN视作是向量化的产物并取得了巨大的成功。
除了观念的变化，
近年来
语言模型技术的发展\cite{Bengio:2003:NPL:944919.944966,DBLP:conf/interspeech/MikolovKBCK10,Kim:2016:CNL:3016100.3016285,jean-EtAl:2015:ACL-IJCNLP}以及
多任务学习技术的发展\cite{guo-EtAl:2016:COLING1,clark-EtAl:2018:EMNLP}
也为ELMo的成功提供了保障。
沿着\elmochinesetranslation 的思路，
Radford等人在2018年的文献\inlinecite{gpt1}中提出使用
单向Transformer建模上文的模型 --- Generative Pre-Training（简称GPT）。
除了描述上下方式的不同，
GPT的最大特点是
在特定任务上对预训练的语言模型进行\finetunechinesetranslation。
这种方法进一步提升了模型性能。

\begin{table}[t]
	\bicaption{}{上下文相关词向量算法的比较。}{Table $\!$}{A comparison on the contextualized word embedding algorithm.\label{tbl:intro:context-emb}}
	\vspace{0.5em}\centering\wuhao
	\begin{tabular}{r lp{3.8cm}lc}
		\toprule[1.5pt]
		模型名称 &上下文表示 & 目标词与上下文的关系 & 用法 & Multi-NLI结果 \\
		\midrule[1pt]
		CoVe & 双向LSTM &  机器翻译& 特征 & - \\
		ELMo & 双向多层LSTM &  双向语言模型 & 特征 & 79.6/79.3 \\
		ELMo+ & 双向多层Gated CNN  & 双向语言模型 & 特征& 78.3/77.9 \\
		& 双向Transformer  & 双向语言模型 & 特征& 79.4/78.3 \\
		GPT & Transformer   & 单向语言模型 & 初始化& 81.8/- \\
		BERT & Transformer & 词级别完形填空，句子级完形填空 & 初始化 & 86.7/85.9
\\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}

\paragraph{BERT}
2018年下半年，Devlin等人基于GPT预训练的思想，
在文献\inlinecite{DBLP:journals/corr/abs-1810-04805}中提出使用
自注意网络（Bidirectional Encoder Representations from Transformers，简称BERT）建模目标词的左侧和右侧的上下文。
通过将目标词遮罩
并使用
词级别完形填空（或称遮罩语言模型，masked language model）
和句子级完形填空的学习目标训练双向自注意网络，
从而取得了
11项任务的大幅度性能提升，获得了超越ELMo与GPT的效果。

相对ELMo与GPT，BERT对于建模词义分布式假设
做出的改进是多方面的。
从描述上下文的角度，
BERT利用双向自注意网络的特点，真正地描述了``双向''的上下文。
从描述目标词与上下的角度，
BERT提出的两个学习目标如下：
\begin{itemize}
	\item 词级别完形填空：随机将句子中的词替换为一个特殊标记，并用
	这个位置的上下文相关词向量预测原来的词；
	\item 句子级完形填空：将两个句子连接起来，并预测第二个句子
	是不是第一个句子的后续句子。值得一提的是，这种句子级完形填空
	的分类问题是在词级别上进行预测与学习的。
\end{itemize}

这种方式与语言模型有很大差异。
但与Collobert等人2011年的工作\inlinecite{Collobert:2011:NLP:1953048.2078186}
有相似之处。

%\paragraph{上下文相关词向量的分析工作}
通过上述讨论不难看出，
虽然产生巨大变革，
上下文相关词向量仍可以
套用建模词义分布式假设的两个维度：
上下文以及上下文和目标词的关系
进行解释。
表\ref{tbl:intro:context-emb}从
这两个维度对上述上下文相关词向量方法进行总结。
%同时，
%需要注意的是现今主要的上下文相关词向量算法
%都可以在其上下文无关词向量算法中找到对应。
%能否借鉴上下文无关词向量算法中建模词义分布式假设的方法，
%以及如何将上下文无关词向量的一系列研究成果与
%这些上下文相关的词向量算法进行结合
%都是值得研究的问题。
然而，上下文相关词向量的研究不应只局限于表\ref{tbl:intro:context-emb}。
新的上下文表示方法以及新的建模目标词与上下文关系的模型都是值得探索的。

\subsection{基于词向量的语言分析技术}[Language Analysis with Word Embeddings]\label{sec:intro:review:lang-anal}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth,trim=0 0 3cm 1cm,clip]{intro/structure}
	\bicaption{}{一个语言分析的例子}
	{Fig.~$\!$}{An example of language analysis\label{fig:intro:pipeline}}
\end{figure}

包括分词、词性标注、命名实体识别、句法分析
在内的语言分析任务是
下游自然语言处理任务
的基础（如图\ref{fig:intro:pipeline}所示）。
语言分析模型的性能很大程度上影响了下游任务的准确率。
因而吸引了大量科研兴趣。
在接下来的章节中，
本章将首先从词法分析（\S\ref{sec:intro:review:lang-anal:lex}）
和句法分析（\S\ref{sec:intro:review:lang-anal:syn}）
两个方面对于当前研究进行回顾。
然后讨论表示学习与基于结构预测的语言分析的关系（\S\ref{sec:intro:review:lang-anal:relation}）。

\subsubsection{基于词向量的词法分析}\label{sec:intro:review:lang-anal:lex}
一般认为，
词法分析包括中文分词、词性标注以及命名实体识别。
其中，中文分词指的是把中文的原始句子分割成词序列的过程。
举例来讲，对于句子``浦东开发与经济建设同步''进行分词可以得到``
浦东\ 开发\ 与\ 经济\ 建设\ 同步''的词序列。
词性标注指的是为句子中的每个词标注词性的过程。
词性主要包括名词、动词、形容词、副词、介词、代词等，是一个词的句法类别的概括。
命名实体识别指的是识别句子中的实体并给实体打标签的过程，
以``Michael Jordan is a professor at Berkeley''为例，
命名实体识别将连续的词片段``Michael Jordan''识别为人名，将``Berkeley''识别为机构名。
根据输出结构的不同，
这三种问题可以归类为切分（分词和命名实体识别）以及序列（词性标注）
两类问题。

对于切分问题，
最直接的建模方法是将其转换为标注边界的序列问题\cite{Xue:2003:CWS:1119250.1119278,Ando:2005:FLP:1046920.1194905}。
以``Michael Jordan is a professor at Berkeley''的命名实体识别为例，
这种方法给输入的7个词打``B-PER, E-PER, O, O, O, O, S-ORG''标签。
这种建模范式将三种词法分析进行了统一。
从这种统一的序列标注的角度来看，
词法分析模型可以拆解为两部分：1）对输入的词进行表示；2）对输出的序列结构进行建模。
Collobert等人在2011年的文献\inlinecite{Collobert:2011:NLP:1953048.2078186}中最早
将词向量输入卷积神经网络从而对于词
及其周围的上下文进行表示。
文献\inlinecite{Collobert:2011:NLP:1953048.2078186}使用简单分类的方式对结构进行建模。
Huang等人在2015年的文献\inlinecite{DBLP:journals/corr/HuangXY15}将
基于LSTM的上下文表示引入词法分析，并使用CRF建模结构。
Lample等人在2016年的文献\inlinecite{lample-EtAl:2016:N16-1}
中强调了使用基于字的模型对词进行表示，从而取得显著的性能提升。
Peters等人在2017年的文献\inlinecite{peters-EtAl:2017:Long}中提出
将语言模型的输出作为词表示的补充，从而获得进一步的性能提升。

从基于词向量的一系列词法分析工作来看，
模型性能的提升总是伴随着词表示方法的进步。
Reimers与Gurevych也在2017年文献\inlinecite{reimers-gurevych:2017:EMNLP2017}
中指出词法分析模型的性能与词向量选取的相关性。
但文献\inlinecite{reimers-gurevych:2017:EMNLP2017}
显示合理建模结构仍有其意义。

除了使用CRF对结构建模，
前人工作也尝试了在词法分析中使用基于转移以及全局解码的方法\cite{cai-zhao:2016:P16-1,zhou-EtAl:2017:EMNLP2017}。
可以说，加强词表示是词法分析性能提升的重要途径。
在词表示相同的条件下，合理地建模结构也能带来性能的提升。

\subsubsection{基于词向量的句法分析}\label{sec:intro:review:lang-anal:syn}
主流句法表示形式有两种：短语句法和依存句法。
短语结构句法描述短语的句法功能以及若干短语组合形成新的短语的过程。
依存句法则描述句子中词与词的修饰关系。
相较短语结构句法，依存句法具有形式简单易懂、可以刻画交叉关系等优势。
因而，本文将句法分析的重点放在依存句法上。

依存句法分析的算法主要有两类：基于图的方法与基于转移的方法。
基于图的方法将依存句法分析转化为一个寻找最大生成树问题。
如果打分函数定义在有限阶的弧上，
寻找最大生成树的问题可以通过动态规划进行求解\cite{mcdonald2006online}。
基于转移的方法则将依存句法分析转化为一个最优移进-规约序列的搜索问题\cite{nivre2008algorithms}。

Chen与Manning在2014年的文献\inlinecite{chen-manning:2014:EMNLP2014}中
首次将词向量应用于基于转移的依存句法中。
文献\inlinecite{chen-manning:2014:EMNLP2014}的方法在提出之处尚无法与传统基于
特征的方法抗衡。
但Weiss等人在2015年的文献\inlinecite{weiss-EtAl:2015:ACL-IJCNLP}发展了文献\inlinecite{chen-manning:2014:EMNLP2014}
的方法，并通过多层网络强化了词表示并取得了当时最优的性能。
然而，
文献\inlinecite{weiss-EtAl:2015:ACL-IJCNLP}的方法非常依赖调参且稳定性较差。
Dyer等人在2015年的文献\inlinecite{dyer-EtAl:2015:ACL-IJCNLP}
中提出的通过LSTM建模词的上下文的方法真正给基于词向量的句法分析带来了稳定的提升，
进而证明有效表示词对于句法分析的重要性。

上述工作主要针对基于转移的方法。
Kiperwasser与Goldberg在2016年的文献\inlinecite{TACL885}
中首次将词向量应用于基于图的依存句法分析中。
文献\inlinecite{TACL885}使用双向LSTM对上下文合理地建模
并在此基础上使用一阶Esiner算法寻找最大生成树。
文献\inlinecite{TACL885}获得了相对基于转移的方法更好的性能。
而Dozat与Manning在2017年的文献\inlinecite{DBLP:journals/corr/DozatM16}
中进一步发展了文献文献\inlinecite{TACL885}的方法。
相较文献\inlinecite{TACL885}，文献\inlinecite{DBLP:journals/corr/DozatM16}
强化了上下文表示并弱化结构预测算法。
其采用一种贪心的近似算法计算最大生成树。
这种简化为模型性能带来了显著提升。
之后，Dozat等人在2017年的文献\inlinecite{dozat-qi-manning:2017:K17-3}中
加入基于字的词表示，进一步提升了性能。

通过回顾前人基于词向量的句法分析方法，
本文将其性能提升的关键总结为
对于词表示的强化。
这一观察与前一节相符，
即词表示能力的提升是句法分析性能提升的关键。

\subsubsection{表示学习与基于结构预测的语言分析的关系}\label{sec:intro:review:lang-anal:relation}
从结构预测的观点看待这些语言分析问题，
每种问题都有特殊的结构与之对应。
其中，分词、命名实体识别对应分割结构预测问题；
词性标注对应序列结构预测问题；
句法分析对应树结构预测问题。
在神经网络兴盛之前，
研究人员热衷于对自然语言结构进行复杂的建模。
这类研究趋势可以进行如下总结：
1）\textit{强调全局解码}：从最大熵马尔科夫模型\cite{McCallum:2000:MEM:645529.658277}
到条件随机场模型\cite{Lafferty:2001:CRF:645530.655813}
是这一研究范式的代表。
这类研究强调在全局条件下无偏地求最优结构（应用于测试阶段）
以及无偏地计算边缘概率（应用于训练阶段）。
2）\textit{强调高阶结构}：从句法分析的
一阶Eisner算法\cite{eisner-1996-coling}到
二阶甚至更高阶的最大生成树算法\cite{mcdonald2006online}
是这一研究范式的代表。
同时，
这种思想也激发了以牺牲精确解码换取任意阶特征
的非精确解码算法\cite{zhang-clark:cl:2011}。

全局解码与高阶特征在神经网络兴起之前
受到推崇的一大原因是稀疏的词汇化特征表达能力不足。
实际，早在2008年，Liang等人就在文献\inlinecite{liang:icml08}
中指出解码的复杂程度与表示复杂程度存在某种折中。
即简单的解码与复杂的特征表示可以达到复杂的解码与简单的特征表示
相同的性能。

Liang等人的观点在基于神经网络的语言分析中得到充分的验证。
基于神经网络的语言分析模型的一大特点就是
表示模型无一例外非常复杂。
当前性能最优的词性标注模型\cite{ma-hovy:2016:P16-1,lample-EtAl:2016:N16-1}
使用LSTM或CNN建模词，
并使用多层双向LSTM建模上下文。
性能最优的分词模型\cite{ma-ganchev-weiss:2018:EMNLP}
使用也双向LSTM建模上下文。
性能最优的依存句法分析器\cite{DBLP:journals/corr/DozatM16}也采用类似的上下文表示。
可以说，
双向LSTM已经成为诸多语言分析模型的标准配置。
伴随着LSTM的大规模应用，
全局解码与高阶特征或被抛弃或被证明对于性能影响较小。
Dozat与Manning在2016年的文献\inlinecite{DBLP:journals/corr/DozatM16}中
提出的当前性能最优的深度双仿射句法分析器
是这一最新科研范式的最佳例证。
深度双仿射句法分析器完全抛弃了
稀疏特征时代标配的Esiner算法（或更高阶的最大生成树算法），
将依存句法分析化简为分类问题。
相同的，
Ma等人在2018年的文献\inlinecite{ma-ganchev-weiss:2018:EMNLP}中
提出的
性能最优的中文分词器也只依赖于字级别的分类，
而抛弃了稀疏特征时代的``标配''条件随机场模型。

基于上述分析，
强化基于神经网络的语言分析模型中的表示模型
是提高语言分析性能的重要而有效的手段。
上下文相关词向量作为一种
有效的表示模型有望提高
语言分析模型的性能。

\section{本文的研究内容及章节安排}[Content and Outlines]
\begin{figure}[t]
	\centering
	%	\includegraphics[width=\columnwidth,trim=0 0 3cm 5cm,clip]{intro/thesis_struct}
	\includegraphics[width=\columnwidth,trim=0 0 5.5cm 7cm,clip]{intro/thesis_struct}
	\bicaption{}{论文总体框架}
	{Fig.~$\!$}{The organization of the thesis\label{fig:intro:thesis-struct}}
\end{figure}

本文针对上下文相关词向量在语言分析技术
中的应用开展一系列研究工作。
这些工作按照从词表示，到词法分析，再到句法分析的顺序组织。
具体来讲，本文包含5章。各章节内容组织如下：

在第\ref{chp:intro}章中，
本文介绍了本文课题的研究背景、意义，并对上下文相关词向量
及其在语言分析方面的应用现状
进行概述与分析，
最后对本文主要内容进行了规划。

在第\ref{chp:elmo}章中，
针对现有上下文相关词向量的效率问题，
本文从大部分语言分析模型
仅依赖局部上下文表示出发，
提出了一种
融合相对位置权重的窗口级自注意力机制
并用它建模上下文相关词向量。
实验结果表明，
本文提出的词向量
能够在不损失语言分析精度的
条件下三倍提升模型的解码速度。
%第\ref{chp:elmo}章内容
%为后续章节提供词向量表示的基础。

在第\ref{chp:semicrf}章中，
针对语言分析中的切分问题（中文分词与命名实体识别）
对合理的片段（词与实体）表示的依赖，
本文在上下文相关词向量的基础上
提出一种基于简单拼接的片段表示方法并
将其应用于半-马尔科夫条件随机场中。
在典型切分问题的实验中，
本文基于上下文相关词向量的片段表示有效地提高了模型性能。
通过进一步融合任务相关的上下文表示以及
建模片段级信息的片段向量，本文模型取得了当前最优的性能。
本文第\ref{chp:semicrf}章的工作也
经验性地证明了上下文相关词向量可以通过简单组合表示片段。

在第\ref{chp:seqlabel}章中，
针对上下文相关词向量对多国语句法分析作用尚无明确结论的现状，
本文提出在多国语句法分析中使用上下文相关词向量
并在大规模树库上验证上下文相关词向量的有效性。
除了获得稳定且显著的性能提升，
本文针对提升的原因进行了详细的分析。
大量分析实验表明，
性能提升的主要原因是上下文相关词向量
通过对于未登录词词形的更好的建模
有效地提升了未登录词的准确率。
基于这一分析结果，
本文在资源稀缺的模拟数据中进行了实验，
并经验性地证明少量
标注数据与上下文相关词向量
配合可以获得很好的句法分析性能。

在第\ref{chp:distill}章中，
针对使用上下文相关词向量的句法分析参数过多、运行速度较慢的问题，
本文提出一种结合探索机制的知识蒸馏算法，
以将基于上下文相关词向量的复杂模型
用不使用相应词向量的简单模型进行近似，
从而在不显著降低性能的情况下提高句法分析速度。
实验结果表明，
本文提出的方法
在损失少量句法分析准确率的情况下，
近十倍地提升了速度。

本文各章节之间逻辑关系如图\ref{fig:intro:thesis-struct}所示。
其中，
第\ref{chp:elmo}章的词表示的工作为后续章节提供支撑，
第\ref{chp:semicrf}章在第\ref{chp:elmo}章的基础上研究了词法分析
并通过词法分析探索了第\ref{chp:elmo}章词表示的组合特性。
第\ref{chp:seqlabel}章在第\ref{chp:elmo}章的基础上研究了句法分析
并详细分析了第\ref{chp:elmo}章词表示与句法分析的关系。
第\ref{chp:distill}章以为第\ref{chp:elmo}章和第\ref{chp:seqlabel}章
为基础，将这两章的复杂模型进行速度优化从而使之
获得更广泛的应用。